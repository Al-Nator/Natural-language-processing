{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лабораторная работа №3: **Генерация текста с использованием рекуррентных нейронных сетей (RNN)**\n",
    "\n",
    "#### Теоретическая часть\n",
    "\n",
    "**Рекуррентные нейронные сети (RNN)** – это вид нейронных сетей, специально разработанный для работы с последовательностями данных, такими как текст. Они способны запоминать предыдущие состояния и использовать эту информацию для прогнозирования следующего элемента последовательности.\n",
    "\n",
    "#### Практическая задача\n",
    "\n",
    "Создайте модель RNN для генерации текста на основе корпуса данных литературных произведений:\n",
    "\n",
    "1. Загрузите и предобработайте данные: токенизируйте текст, создайте словарь и преобразуйте текст в индексы.\n",
    "2. Определите архитектуру RNN: количество слоев, размер скрытого слоя, функции активации.\n",
    "3. Обучите модель на тренировочном наборе данных.\n",
    "4. Проверьте способность модели генерировать осмысленный текст.\n",
    "5. Оцените качество генерируемого текста вручную и с помощью метрик, таких как BLEU.\n",
    "\n",
    "#### Указания по выполнению\n",
    "\n",
    "1. Используйте библиотеку `TensorFlow` или `PyTorch` для построения и обучения модели.\n",
    "2. Экспериментируйте с различными гиперпараметрами модели (размер батча, количество эпох, скорость обучения).\n",
    "3. Проведите сравнение производительности вашей модели с другими подходами, такими как LSTM или GRU.\n",
    "4. Подготовьте отчет, включающий код, примеры сгенерированного текста и анализ результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import string\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Загрузите и предобработайте данные: токенизируйте текст, создайте словарь и преобразуйте текст в индексы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    }
   ],
   "source": [
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря: 52494\n"
     ]
    }
   ],
   "source": [
    "file_ids = gutenberg.fileids()\n",
    "texts_tokens = {} \n",
    "\n",
    "for fileid in file_ids:\n",
    "    text = gutenberg.raw(fileid)\n",
    "    tokens = [token.lower() for token in word_tokenize(text) if token not in string.punctuation]\n",
    "    texts_tokens[fileid] = tokens\n",
    "\n",
    "all_tokens = [token for tokens in texts_tokens.values() for token in tokens]\n",
    "vocab = {token: idx for idx, token in enumerate(sorted(set(all_tokens)))}\n",
    "\n",
    "print(\"Размер словаря:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18621, 11210, 27723, 8239, 1914, 50340, 25779, 12151, 25779, 18621, 51871, 23995, 12903, 7047, 39668, 51719, 5517, 13357, 25209, 7047]\n"
     ]
    }
   ],
   "source": [
    "texts_indices = {}\n",
    "for fileid, tokens in texts_tokens.items():\n",
    "    indices = [vocab[token] for token in tokens]\n",
    "    texts_indices[fileid] = indices\n",
    "\n",
    "print(texts_indices[file_ids[0]][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Определите архитектуру RNN: количество слоев, размер скрытого слоя, функции активации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim, \n",
    "            hidden_size=hidden_size, \n",
    "            num_layers=num_layers, \n",
    "            batch_first=True, \n",
    "            nonlinearity='tanh'\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embed = self.embedding(x)  \n",
    "        output, hidden = self.rnn(embed, hidden)  \n",
    "        output = self.fc(output)  \n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2    \n",
    "\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_size, num_layers).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Обучите модель на тренировочном наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = list(texts_indices.keys())[0]\n",
    "data = texts_indices[sample_file]\n",
    "seq_length = 30\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.data = data\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx+self.seq_length]\n",
    "        y = self.data[idx+1: idx+self.seq_length+1]\n",
    "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = TextDataset(data, seq_length)\n",
    "batch_size = 64\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Batch 0, Loss: 10.8809\n",
      "Epoch 1/5, Batch 100, Loss: 6.2510\n",
      "Epoch 1/5, Batch 200, Loss: 6.1821\n",
      "Epoch 1/5, Batch 300, Loss: 5.6563\n",
      "Epoch 1/5, Batch 400, Loss: 5.3170\n",
      "Epoch 1/5, Batch 500, Loss: 5.0477\n",
      "Epoch 1/5, Batch 600, Loss: 4.8388\n",
      "Epoch 1/5, Batch 700, Loss: 4.7114\n",
      "Epoch 1/5, Batch 800, Loss: 4.5842\n",
      "Epoch 1/5, Batch 900, Loss: 4.5493\n",
      "Epoch 1/5, Batch 1000, Loss: 4.4937\n",
      "Epoch 1/5, Batch 1100, Loss: 4.3641\n",
      "Epoch 1/5, Batch 1200, Loss: 4.1875\n",
      "Epoch 1/5, Batch 1300, Loss: 4.1423\n",
      "Epoch 1/5, Batch 1400, Loss: 4.0706\n",
      "Epoch 1/5, Batch 1500, Loss: 3.9702\n",
      "Epoch 1/5, Batch 1600, Loss: 3.9397\n",
      "Epoch 1/5, Batch 1700, Loss: 3.7653\n",
      "Epoch 1/5, Batch 1800, Loss: 3.6708\n",
      "Epoch 1/5, Batch 1900, Loss: 3.6410\n",
      "Epoch 1/5, Batch 2000, Loss: 3.4713\n",
      "Epoch 1/5, Batch 2100, Loss: 3.4618\n",
      "Epoch 1/5, Batch 2200, Loss: 3.3776\n",
      "Epoch 1/5, Batch 2300, Loss: 3.3132\n",
      "Epoch 1/5, Batch 2400, Loss: 3.2149\n",
      "Epoch 1/5, Batch 2500, Loss: 3.0893\n",
      "Epoch 1/5, Batch 2600, Loss: 3.0636\n",
      "Epoch 1 average loss: 4.3042\n",
      "Epoch 2/5, Batch 0, Loss: 2.9249\n",
      "Epoch 2/5, Batch 100, Loss: 2.9179\n",
      "Epoch 2/5, Batch 200, Loss: 2.8683\n",
      "Epoch 2/5, Batch 300, Loss: 2.7018\n",
      "Epoch 2/5, Batch 400, Loss: 2.7561\n",
      "Epoch 2/5, Batch 500, Loss: 2.6243\n",
      "Epoch 2/5, Batch 600, Loss: 2.6779\n",
      "Epoch 2/5, Batch 700, Loss: 2.5262\n",
      "Epoch 2/5, Batch 800, Loss: 2.4876\n",
      "Epoch 2/5, Batch 900, Loss: 2.4904\n",
      "Epoch 2/5, Batch 1000, Loss: 2.4389\n",
      "Epoch 2/5, Batch 1100, Loss: 2.3699\n",
      "Epoch 2/5, Batch 1200, Loss: 2.3801\n",
      "Epoch 2/5, Batch 1300, Loss: 2.2787\n",
      "Epoch 2/5, Batch 1400, Loss: 2.2642\n",
      "Epoch 2/5, Batch 1500, Loss: 2.1876\n",
      "Epoch 2/5, Batch 1600, Loss: 2.1794\n",
      "Epoch 2/5, Batch 1700, Loss: 2.1495\n",
      "Epoch 2/5, Batch 1800, Loss: 2.2359\n",
      "Epoch 2/5, Batch 1900, Loss: 2.0530\n",
      "Epoch 2/5, Batch 2000, Loss: 1.9539\n",
      "Epoch 2/5, Batch 2100, Loss: 2.0342\n",
      "Epoch 2/5, Batch 2200, Loss: 1.9456\n",
      "Epoch 2/5, Batch 2300, Loss: 1.9210\n",
      "Epoch 2/5, Batch 2400, Loss: 1.9293\n",
      "Epoch 2/5, Batch 2500, Loss: 1.9052\n",
      "Epoch 2/5, Batch 2600, Loss: 1.9052\n",
      "Epoch 2 average loss: 2.3316\n",
      "Epoch 3/5, Batch 0, Loss: 1.8180\n",
      "Epoch 3/5, Batch 100, Loss: 1.8323\n",
      "Epoch 3/5, Batch 200, Loss: 1.7682\n",
      "Epoch 3/5, Batch 300, Loss: 1.7639\n",
      "Epoch 3/5, Batch 400, Loss: 1.7246\n",
      "Epoch 3/5, Batch 500, Loss: 1.7989\n",
      "Epoch 3/5, Batch 600, Loss: 1.7489\n",
      "Epoch 3/5, Batch 700, Loss: 1.6966\n",
      "Epoch 3/5, Batch 800, Loss: 1.7106\n",
      "Epoch 3/5, Batch 900, Loss: 1.5866\n",
      "Epoch 3/5, Batch 1000, Loss: 1.5408\n",
      "Epoch 3/5, Batch 1100, Loss: 1.6075\n",
      "Epoch 3/5, Batch 1200, Loss: 1.5379\n",
      "Epoch 3/5, Batch 1300, Loss: 1.5433\n",
      "Epoch 3/5, Batch 1400, Loss: 1.5829\n",
      "Epoch 3/5, Batch 1500, Loss: 1.5606\n",
      "Epoch 3/5, Batch 1600, Loss: 1.5739\n",
      "Epoch 3/5, Batch 1700, Loss: 1.4985\n",
      "Epoch 3/5, Batch 1800, Loss: 1.4055\n",
      "Epoch 3/5, Batch 1900, Loss: 1.4508\n",
      "Epoch 3/5, Batch 2000, Loss: 1.3982\n",
      "Epoch 3/5, Batch 2100, Loss: 1.4336\n",
      "Epoch 3/5, Batch 2200, Loss: 1.4719\n",
      "Epoch 3/5, Batch 2300, Loss: 1.3734\n",
      "Epoch 3/5, Batch 2400, Loss: 1.4113\n",
      "Epoch 3/5, Batch 2500, Loss: 1.3595\n",
      "Epoch 3/5, Batch 2600, Loss: 1.3665\n",
      "Epoch 3 average loss: 1.5718\n",
      "Epoch 4/5, Batch 0, Loss: 1.3371\n",
      "Epoch 4/5, Batch 100, Loss: 1.3167\n",
      "Epoch 4/5, Batch 200, Loss: 1.3779\n",
      "Epoch 4/5, Batch 300, Loss: 1.2981\n",
      "Epoch 4/5, Batch 400, Loss: 1.2581\n",
      "Epoch 4/5, Batch 500, Loss: 1.2408\n",
      "Epoch 4/5, Batch 600, Loss: 1.2431\n",
      "Epoch 4/5, Batch 700, Loss: 1.3356\n",
      "Epoch 4/5, Batch 800, Loss: 1.2552\n",
      "Epoch 4/5, Batch 900, Loss: 1.2646\n",
      "Epoch 4/5, Batch 1000, Loss: 1.3022\n",
      "Epoch 4/5, Batch 1100, Loss: 1.3063\n",
      "Epoch 4/5, Batch 1200, Loss: 1.2209\n",
      "Epoch 4/5, Batch 1300, Loss: 1.1632\n",
      "Epoch 4/5, Batch 1400, Loss: 1.2780\n",
      "Epoch 4/5, Batch 1500, Loss: 1.2262\n",
      "Epoch 4/5, Batch 1600, Loss: 1.1992\n",
      "Epoch 4/5, Batch 1700, Loss: 1.2341\n",
      "Epoch 4/5, Batch 1800, Loss: 1.1626\n",
      "Epoch 4/5, Batch 1900, Loss: 1.1500\n",
      "Epoch 4/5, Batch 2000, Loss: 1.2456\n",
      "Epoch 4/5, Batch 2100, Loss: 1.1456\n",
      "Epoch 4/5, Batch 2200, Loss: 1.1856\n",
      "Epoch 4/5, Batch 2300, Loss: 1.1758\n",
      "Epoch 4/5, Batch 2400, Loss: 1.1445\n",
      "Epoch 4/5, Batch 2500, Loss: 1.1743\n",
      "Epoch 4/5, Batch 2600, Loss: 1.2093\n",
      "Epoch 4 average loss: 1.2375\n",
      "Epoch 5/5, Batch 0, Loss: 1.0935\n",
      "Epoch 5/5, Batch 100, Loss: 1.0663\n",
      "Epoch 5/5, Batch 200, Loss: 1.1389\n",
      "Epoch 5/5, Batch 300, Loss: 1.1296\n",
      "Epoch 5/5, Batch 400, Loss: 1.0772\n",
      "Epoch 5/5, Batch 500, Loss: 1.0874\n",
      "Epoch 5/5, Batch 600, Loss: 1.0955\n",
      "Epoch 5/5, Batch 700, Loss: 1.1182\n",
      "Epoch 5/5, Batch 800, Loss: 1.0751\n",
      "Epoch 5/5, Batch 900, Loss: 1.0970\n",
      "Epoch 5/5, Batch 1000, Loss: 1.0663\n",
      "Epoch 5/5, Batch 1100, Loss: 1.0230\n",
      "Epoch 5/5, Batch 1200, Loss: 1.0578\n",
      "Epoch 5/5, Batch 1300, Loss: 1.0264\n",
      "Epoch 5/5, Batch 1400, Loss: 1.0910\n",
      "Epoch 5/5, Batch 1500, Loss: 0.9842\n",
      "Epoch 5/5, Batch 1600, Loss: 1.1422\n",
      "Epoch 5/5, Batch 1700, Loss: 1.0641\n",
      "Epoch 5/5, Batch 1800, Loss: 1.0158\n",
      "Epoch 5/5, Batch 1900, Loss: 1.0935\n",
      "Epoch 5/5, Batch 2000, Loss: 1.0563\n",
      "Epoch 5/5, Batch 2100, Loss: 0.9802\n",
      "Epoch 5/5, Batch 2200, Loss: 1.0281\n",
      "Epoch 5/5, Batch 2300, Loss: 1.0344\n",
      "Epoch 5/5, Batch 2400, Loss: 1.0457\n",
      "Epoch 5/5, Batch 2500, Loss: 0.9453\n",
      "Epoch 5/5, Batch 2600, Loss: 1.0523\n",
      "Epoch 5 average loss: 1.0644\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        current_batch_size = inputs.size(0)\n",
    "        hidden = model.init_hidden(current_batch_size).to(device)\n",
    "\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        outputs = outputs.reshape(-1, vocab_size)\n",
    "        targets = targets.reshape(-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1} average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Проверьте способность модели генерировать осмысленный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированный текст:\n",
      " Alice was beginning to get very tired they were gone so very painful when the baby was fetched on and for it seemed as if ever willing to be constantly with her at possible and trying for their state she never met with her without carrying on isabella 's happiness had been prepared to have received a\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, start_text, vocab, inv_vocab, device, gen_length=10):\n",
    "    model.eval()\n",
    "    tokens = word_tokenize(start_text)\n",
    "    input_indices = [vocab.get(token, 0) for token in tokens]\n",
    "    \n",
    "    input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(input_tensor.size(0)).to(device)\n",
    "    \n",
    "    generated_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(gen_length):\n",
    "        outputs, hidden = model(input_tensor, hidden)\n",
    "        logits = outputs[:, -1, :] \n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated_tokens.append(inv_vocab[next_token_idx])\n",
    "        input_tensor = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
    "    \n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "inv_vocab = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "start_text = \"Alice was beginning to get very tired\"\n",
    "generated = generate_text(model, start_text, vocab, inv_vocab, device, gen_length=50)\n",
    "print(\"Сгенерированный текст:\\n\", generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерированный текст демонстрирует, что модель способна продолжать текст на основе заданного начала. Однако, как видно, результат может быть несколько бессвязным или фрагментированным, что часто бывает при ограниченном обучении или недостаточной мощности модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 1/5, Batch 0, Loss: 10.8700\n",
      "LSTM Epoch 1/5, Batch 100, Loss: 6.4026\n",
      "LSTM Epoch 1/5, Batch 200, Loss: 6.2967\n",
      "LSTM Epoch 1/5, Batch 300, Loss: 6.1839\n",
      "LSTM Epoch 1/5, Batch 400, Loss: 6.3843\n",
      "LSTM Epoch 1/5, Batch 500, Loss: 6.2551\n",
      "LSTM Epoch 1/5, Batch 600, Loss: 6.0833\n",
      "LSTM Epoch 1/5, Batch 700, Loss: 5.9969\n",
      "LSTM Epoch 1/5, Batch 800, Loss: 5.9000\n",
      "LSTM Epoch 1/5, Batch 900, Loss: 5.8403\n",
      "LSTM Epoch 1/5, Batch 1000, Loss: 5.6221\n",
      "LSTM Epoch 1/5, Batch 1100, Loss: 5.7723\n",
      "LSTM Epoch 1/5, Batch 1200, Loss: 5.5979\n",
      "LSTM Epoch 1/5, Batch 1300, Loss: 5.5823\n",
      "LSTM Epoch 1/5, Batch 1400, Loss: 5.5244\n",
      "LSTM Epoch 1/5, Batch 1500, Loss: 5.3660\n",
      "LSTM Epoch 1/5, Batch 1600, Loss: 5.3731\n",
      "LSTM Epoch 1/5, Batch 1700, Loss: 5.4039\n",
      "LSTM Epoch 1/5, Batch 1800, Loss: 5.3224\n",
      "LSTM Epoch 1/5, Batch 1900, Loss: 5.2762\n",
      "LSTM Epoch 1/5, Batch 2000, Loss: 5.2308\n",
      "LSTM Epoch 1/5, Batch 2100, Loss: 5.1516\n",
      "LSTM Epoch 1/5, Batch 2200, Loss: 5.1651\n",
      "LSTM Epoch 1/5, Batch 2300, Loss: 5.1178\n",
      "LSTM Epoch 1/5, Batch 2400, Loss: 5.1369\n",
      "LSTM Epoch 1/5, Batch 2500, Loss: 5.0998\n",
      "LSTM Epoch 1/5, Batch 2600, Loss: 5.1485\n",
      "LSTM Epoch 1 average loss: 5.6672\n",
      "LSTM Epoch 2/5, Batch 0, Loss: 5.0362\n",
      "LSTM Epoch 2/5, Batch 100, Loss: 4.9001\n",
      "LSTM Epoch 2/5, Batch 200, Loss: 4.9048\n",
      "LSTM Epoch 2/5, Batch 300, Loss: 4.8979\n",
      "LSTM Epoch 2/5, Batch 400, Loss: 4.8390\n",
      "LSTM Epoch 2/5, Batch 500, Loss: 4.7656\n",
      "LSTM Epoch 2/5, Batch 600, Loss: 4.8830\n",
      "LSTM Epoch 2/5, Batch 700, Loss: 4.7906\n",
      "LSTM Epoch 2/5, Batch 800, Loss: 4.7855\n",
      "LSTM Epoch 2/5, Batch 900, Loss: 4.8163\n",
      "LSTM Epoch 2/5, Batch 1000, Loss: 4.6630\n",
      "LSTM Epoch 2/5, Batch 1100, Loss: 4.6926\n",
      "LSTM Epoch 2/5, Batch 1200, Loss: 4.6174\n",
      "LSTM Epoch 2/5, Batch 1300, Loss: 4.7213\n",
      "LSTM Epoch 2/5, Batch 1400, Loss: 4.5235\n",
      "LSTM Epoch 2/5, Batch 1500, Loss: 4.6016\n",
      "LSTM Epoch 2/5, Batch 1600, Loss: 4.5634\n",
      "LSTM Epoch 2/5, Batch 1700, Loss: 4.5377\n",
      "LSTM Epoch 2/5, Batch 1800, Loss: 4.6939\n",
      "LSTM Epoch 2/5, Batch 1900, Loss: 4.4374\n",
      "LSTM Epoch 2/5, Batch 2000, Loss: 4.3498\n",
      "LSTM Epoch 2/5, Batch 2100, Loss: 4.4762\n",
      "LSTM Epoch 2/5, Batch 2200, Loss: 4.3021\n",
      "LSTM Epoch 2/5, Batch 2300, Loss: 4.3069\n",
      "LSTM Epoch 2/5, Batch 2400, Loss: 4.1940\n",
      "LSTM Epoch 2/5, Batch 2500, Loss: 4.2493\n",
      "LSTM Epoch 2/5, Batch 2600, Loss: 4.2467\n",
      "LSTM Epoch 2 average loss: 4.6123\n",
      "LSTM Epoch 3/5, Batch 0, Loss: 4.1035\n",
      "LSTM Epoch 3/5, Batch 100, Loss: 4.1775\n",
      "LSTM Epoch 3/5, Batch 200, Loss: 4.1780\n",
      "LSTM Epoch 3/5, Batch 300, Loss: 4.0911\n",
      "LSTM Epoch 3/5, Batch 400, Loss: 4.0312\n",
      "LSTM Epoch 3/5, Batch 500, Loss: 4.0437\n",
      "LSTM Epoch 3/5, Batch 600, Loss: 3.9536\n",
      "LSTM Epoch 3/5, Batch 700, Loss: 3.9457\n",
      "LSTM Epoch 3/5, Batch 800, Loss: 3.9036\n",
      "LSTM Epoch 3/5, Batch 900, Loss: 3.8703\n",
      "LSTM Epoch 3/5, Batch 1000, Loss: 3.7384\n",
      "LSTM Epoch 3/5, Batch 1100, Loss: 3.8378\n",
      "LSTM Epoch 3/5, Batch 1200, Loss: 3.6736\n",
      "LSTM Epoch 3/5, Batch 1300, Loss: 3.6193\n",
      "LSTM Epoch 3/5, Batch 1400, Loss: 3.7108\n",
      "LSTM Epoch 3/5, Batch 1500, Loss: 3.6687\n",
      "LSTM Epoch 3/5, Batch 1600, Loss: 3.5670\n",
      "LSTM Epoch 3/5, Batch 1700, Loss: 3.4553\n",
      "LSTM Epoch 3/5, Batch 1800, Loss: 3.6215\n",
      "LSTM Epoch 3/5, Batch 1900, Loss: 3.5323\n",
      "LSTM Epoch 3/5, Batch 2000, Loss: 3.4619\n",
      "LSTM Epoch 3/5, Batch 2100, Loss: 3.4604\n",
      "LSTM Epoch 3/5, Batch 2200, Loss: 3.3812\n",
      "LSTM Epoch 3/5, Batch 2300, Loss: 3.3574\n",
      "LSTM Epoch 3/5, Batch 2400, Loss: 3.1886\n",
      "LSTM Epoch 3/5, Batch 2500, Loss: 3.1861\n",
      "LSTM Epoch 3/5, Batch 2600, Loss: 3.1784\n",
      "LSTM Epoch 3 average loss: 3.6772\n",
      "LSTM Epoch 4/5, Batch 0, Loss: 3.1249\n",
      "LSTM Epoch 4/5, Batch 100, Loss: 3.1088\n",
      "LSTM Epoch 4/5, Batch 200, Loss: 3.0671\n",
      "LSTM Epoch 4/5, Batch 300, Loss: 3.0212\n",
      "LSTM Epoch 4/5, Batch 400, Loss: 3.0063\n",
      "LSTM Epoch 4/5, Batch 500, Loss: 2.9074\n",
      "LSTM Epoch 4/5, Batch 600, Loss: 2.8927\n",
      "LSTM Epoch 4/5, Batch 700, Loss: 2.8353\n",
      "LSTM Epoch 4/5, Batch 800, Loss: 2.7782\n",
      "LSTM Epoch 4/5, Batch 900, Loss: 2.8415\n",
      "LSTM Epoch 4/5, Batch 1000, Loss: 2.7388\n",
      "LSTM Epoch 4/5, Batch 1100, Loss: 2.7432\n",
      "LSTM Epoch 4/5, Batch 1200, Loss: 2.7582\n",
      "LSTM Epoch 4/5, Batch 1300, Loss: 2.6686\n",
      "LSTM Epoch 4/5, Batch 1400, Loss: 2.6559\n",
      "LSTM Epoch 4/5, Batch 1500, Loss: 2.6799\n",
      "LSTM Epoch 4/5, Batch 1600, Loss: 2.6854\n",
      "LSTM Epoch 4/5, Batch 1700, Loss: 2.6705\n",
      "LSTM Epoch 4/5, Batch 1800, Loss: 2.5153\n",
      "LSTM Epoch 4/5, Batch 1900, Loss: 2.5987\n",
      "LSTM Epoch 4/5, Batch 2000, Loss: 2.4737\n",
      "LSTM Epoch 4/5, Batch 2100, Loss: 2.4990\n",
      "LSTM Epoch 4/5, Batch 2200, Loss: 2.4524\n",
      "LSTM Epoch 4/5, Batch 2300, Loss: 2.5081\n",
      "LSTM Epoch 4/5, Batch 2400, Loss: 2.3668\n",
      "LSTM Epoch 4/5, Batch 2500, Loss: 2.4188\n",
      "LSTM Epoch 4/5, Batch 2600, Loss: 2.3663\n",
      "LSTM Epoch 4 average loss: 2.7145\n",
      "LSTM Epoch 5/5, Batch 0, Loss: 2.2986\n",
      "LSTM Epoch 5/5, Batch 100, Loss: 2.2413\n",
      "LSTM Epoch 5/5, Batch 200, Loss: 2.2922\n",
      "LSTM Epoch 5/5, Batch 300, Loss: 2.3083\n",
      "LSTM Epoch 5/5, Batch 400, Loss: 2.2436\n",
      "LSTM Epoch 5/5, Batch 500, Loss: 2.2010\n",
      "LSTM Epoch 5/5, Batch 600, Loss: 2.1450\n",
      "LSTM Epoch 5/5, Batch 700, Loss: 2.0856\n",
      "LSTM Epoch 5/5, Batch 800, Loss: 2.1454\n",
      "LSTM Epoch 5/5, Batch 900, Loss: 2.2815\n",
      "LSTM Epoch 5/5, Batch 1000, Loss: 2.0915\n",
      "LSTM Epoch 5/5, Batch 1100, Loss: 2.0258\n",
      "LSTM Epoch 5/5, Batch 1200, Loss: 2.0145\n",
      "LSTM Epoch 5/5, Batch 1300, Loss: 2.0471\n",
      "LSTM Epoch 5/5, Batch 1400, Loss: 2.1144\n",
      "LSTM Epoch 5/5, Batch 1500, Loss: 2.0096\n",
      "LSTM Epoch 5/5, Batch 1600, Loss: 1.9253\n",
      "LSTM Epoch 5/5, Batch 1700, Loss: 1.9078\n",
      "LSTM Epoch 5/5, Batch 1800, Loss: 1.8862\n",
      "LSTM Epoch 5/5, Batch 1900, Loss: 1.8442\n",
      "LSTM Epoch 5/5, Batch 2000, Loss: 1.8651\n",
      "LSTM Epoch 5/5, Batch 2100, Loss: 1.7633\n",
      "LSTM Epoch 5/5, Batch 2200, Loss: 1.9544\n",
      "LSTM Epoch 5/5, Batch 2300, Loss: 1.7865\n",
      "LSTM Epoch 5/5, Batch 2400, Loss: 1.8024\n",
      "LSTM Epoch 5/5, Batch 2500, Loss: 1.8313\n",
      "LSTM Epoch 5/5, Batch 2600, Loss: 1.7905\n",
      "LSTM Epoch 5 average loss: 2.0094\n"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embed = self.embedding(x)\n",
    "        output, hidden = self.lstm(embed, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "        return (h0, c0)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_lstm = LSTMModel(vocab_size, embedding_dim, hidden_size, num_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model_lstm.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "\n",
    "model_lstm.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        current_batch_size = inputs.size(0)\n",
    "        hidden = model_lstm.init_hidden(current_batch_size)\n",
    "        hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "        outputs, hidden = model_lstm(inputs, hidden)\n",
    "        outputs = outputs.reshape(-1, vocab_size)\n",
    "        targets = targets.reshape(-1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"LSTM Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"LSTM Epoch {epoch+1} average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированный текст LSTM:\n",
      " Alice was beginning to get very tired of his affections were he could returning her but with many people -- no present to sit for the help of a little deal of intimacy first in the drawing-room\n"
     ]
    }
   ],
   "source": [
    "def generate_text_lstm(model, start_text, vocab, inv_vocab, device, gen_length=30):\n",
    "    model.eval()\n",
    "    tokens = word_tokenize(start_text)\n",
    "    input_indices = [vocab.get(token, 0) for token in tokens]\n",
    "    input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(input_tensor.size(0))\n",
    "    hidden = (hidden[0].to(device), hidden[1].to(device))\n",
    "    generated_tokens = tokens.copy()\n",
    "    for _ in range(gen_length):\n",
    "        outputs, hidden = model(input_tensor, hidden)\n",
    "        logits = outputs[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated_tokens.append(inv_vocab[next_token_idx])\n",
    "        input_tensor = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "generated_lstm = generate_text_lstm(model_lstm, \"Alice was beginning to get very tired\", vocab, inv_vocab, device, gen_length=30)\n",
    "print(\"Сгенерированный текст LSTM:\\n\", generated_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU Epoch 1/5, Batch 0, Loss: 10.8745\n",
      "GRU Epoch 1/5, Batch 100, Loss: 6.2945\n",
      "GRU Epoch 1/5, Batch 200, Loss: 6.2676\n",
      "GRU Epoch 1/5, Batch 300, Loss: 6.2466\n",
      "GRU Epoch 1/5, Batch 400, Loss: 6.1190\n",
      "GRU Epoch 1/5, Batch 500, Loss: 6.0919\n",
      "GRU Epoch 1/5, Batch 600, Loss: 6.1945\n",
      "GRU Epoch 1/5, Batch 700, Loss: 5.9153\n",
      "GRU Epoch 1/5, Batch 800, Loss: 5.8092\n",
      "GRU Epoch 1/5, Batch 900, Loss: 5.8010\n",
      "GRU Epoch 1/5, Batch 1000, Loss: 5.7039\n",
      "GRU Epoch 1/5, Batch 1100, Loss: 5.7064\n",
      "GRU Epoch 1/5, Batch 1200, Loss: 5.6133\n",
      "GRU Epoch 1/5, Batch 1300, Loss: 5.5367\n",
      "GRU Epoch 1/5, Batch 1400, Loss: 5.4066\n",
      "GRU Epoch 1/5, Batch 1500, Loss: 5.3310\n",
      "GRU Epoch 1/5, Batch 1600, Loss: 5.3405\n",
      "GRU Epoch 1/5, Batch 1700, Loss: 5.2307\n",
      "GRU Epoch 1/5, Batch 1800, Loss: 5.1176\n",
      "GRU Epoch 1/5, Batch 1900, Loss: 4.8999\n",
      "GRU Epoch 1/5, Batch 2000, Loss: 4.9798\n",
      "GRU Epoch 1/5, Batch 2100, Loss: 4.8942\n",
      "GRU Epoch 1/5, Batch 2200, Loss: 4.7802\n",
      "GRU Epoch 1/5, Batch 2300, Loss: 4.7155\n",
      "GRU Epoch 1/5, Batch 2400, Loss: 4.6195\n",
      "GRU Epoch 1/5, Batch 2500, Loss: 4.5890\n",
      "GRU Epoch 1/5, Batch 2600, Loss: 4.4820\n",
      "GRU Epoch 1 average loss: 5.4862\n",
      "GRU Epoch 2/5, Batch 0, Loss: 4.4248\n",
      "GRU Epoch 2/5, Batch 100, Loss: 4.2265\n",
      "GRU Epoch 2/5, Batch 200, Loss: 4.2490\n",
      "GRU Epoch 2/5, Batch 300, Loss: 4.2201\n",
      "GRU Epoch 2/5, Batch 400, Loss: 4.0897\n",
      "GRU Epoch 2/5, Batch 500, Loss: 3.8048\n",
      "GRU Epoch 2/5, Batch 600, Loss: 3.8078\n",
      "GRU Epoch 2/5, Batch 700, Loss: 3.8359\n",
      "GRU Epoch 2/5, Batch 800, Loss: 3.7475\n",
      "GRU Epoch 2/5, Batch 900, Loss: 3.7077\n",
      "GRU Epoch 2/5, Batch 1000, Loss: 3.5872\n",
      "GRU Epoch 2/5, Batch 1100, Loss: 3.4822\n",
      "GRU Epoch 2/5, Batch 1200, Loss: 3.4888\n",
      "GRU Epoch 2/5, Batch 1300, Loss: 3.2948\n",
      "GRU Epoch 2/5, Batch 1400, Loss: 3.1901\n",
      "GRU Epoch 2/5, Batch 1500, Loss: 3.1285\n",
      "GRU Epoch 2/5, Batch 1600, Loss: 3.0639\n",
      "GRU Epoch 2/5, Batch 1700, Loss: 2.8759\n",
      "GRU Epoch 2/5, Batch 1800, Loss: 2.9474\n",
      "GRU Epoch 2/5, Batch 1900, Loss: 2.8335\n",
      "GRU Epoch 2/5, Batch 2000, Loss: 2.7573\n",
      "GRU Epoch 2/5, Batch 2100, Loss: 2.6660\n",
      "GRU Epoch 2/5, Batch 2200, Loss: 2.5898\n",
      "GRU Epoch 2/5, Batch 2300, Loss: 2.5327\n",
      "GRU Epoch 2/5, Batch 2400, Loss: 2.4050\n",
      "GRU Epoch 2/5, Batch 2500, Loss: 2.4062\n",
      "GRU Epoch 2/5, Batch 2600, Loss: 2.3323\n",
      "GRU Epoch 2 average loss: 3.3123\n",
      "GRU Epoch 3/5, Batch 0, Loss: 2.2592\n",
      "GRU Epoch 3/5, Batch 100, Loss: 2.1520\n",
      "GRU Epoch 3/5, Batch 200, Loss: 2.0814\n",
      "GRU Epoch 3/5, Batch 300, Loss: 2.1594\n",
      "GRU Epoch 3/5, Batch 400, Loss: 1.9857\n",
      "GRU Epoch 3/5, Batch 500, Loss: 1.9725\n",
      "GRU Epoch 3/5, Batch 600, Loss: 1.8692\n",
      "GRU Epoch 3/5, Batch 700, Loss: 1.9281\n",
      "GRU Epoch 3/5, Batch 800, Loss: 1.8291\n",
      "GRU Epoch 3/5, Batch 900, Loss: 1.9187\n",
      "GRU Epoch 3/5, Batch 1000, Loss: 1.7542\n",
      "GRU Epoch 3/5, Batch 1100, Loss: 1.7100\n",
      "GRU Epoch 3/5, Batch 1200, Loss: 1.5892\n",
      "GRU Epoch 3/5, Batch 1300, Loss: 1.6126\n",
      "GRU Epoch 3/5, Batch 1400, Loss: 1.6238\n",
      "GRU Epoch 3/5, Batch 1500, Loss: 1.6372\n",
      "GRU Epoch 3/5, Batch 1600, Loss: 1.4962\n",
      "GRU Epoch 3/5, Batch 1700, Loss: 1.5646\n",
      "GRU Epoch 3/5, Batch 1800, Loss: 1.4991\n",
      "GRU Epoch 3/5, Batch 1900, Loss: 1.5026\n",
      "GRU Epoch 3/5, Batch 2000, Loss: 1.4725\n",
      "GRU Epoch 3/5, Batch 2100, Loss: 1.4180\n",
      "GRU Epoch 3/5, Batch 2200, Loss: 1.3376\n",
      "GRU Epoch 3/5, Batch 2300, Loss: 1.4106\n",
      "GRU Epoch 3/5, Batch 2400, Loss: 1.3158\n",
      "GRU Epoch 3/5, Batch 2500, Loss: 1.3538\n",
      "GRU Epoch 3/5, Batch 2600, Loss: 1.2862\n",
      "GRU Epoch 3 average loss: 1.6801\n",
      "GRU Epoch 4/5, Batch 0, Loss: 1.2438\n",
      "GRU Epoch 4/5, Batch 100, Loss: 1.2025\n",
      "GRU Epoch 4/5, Batch 200, Loss: 1.2613\n",
      "GRU Epoch 4/5, Batch 300, Loss: 1.1483\n",
      "GRU Epoch 4/5, Batch 400, Loss: 1.2217\n",
      "GRU Epoch 4/5, Batch 500, Loss: 1.1979\n",
      "GRU Epoch 4/5, Batch 600, Loss: 1.1835\n",
      "GRU Epoch 4/5, Batch 700, Loss: 1.0679\n",
      "GRU Epoch 4/5, Batch 800, Loss: 1.0852\n",
      "GRU Epoch 4/5, Batch 900, Loss: 1.0963\n",
      "GRU Epoch 4/5, Batch 1000, Loss: 1.0971\n",
      "GRU Epoch 4/5, Batch 1100, Loss: 1.0951\n",
      "GRU Epoch 4/5, Batch 1200, Loss: 1.0347\n",
      "GRU Epoch 4/5, Batch 1300, Loss: 1.0272\n",
      "GRU Epoch 4/5, Batch 1400, Loss: 1.0384\n",
      "GRU Epoch 4/5, Batch 1500, Loss: 1.0534\n",
      "GRU Epoch 4/5, Batch 1600, Loss: 0.9945\n",
      "GRU Epoch 4/5, Batch 1700, Loss: 0.9983\n",
      "GRU Epoch 4/5, Batch 1800, Loss: 0.9543\n",
      "GRU Epoch 4/5, Batch 1900, Loss: 0.9924\n",
      "GRU Epoch 4/5, Batch 2000, Loss: 0.9043\n",
      "GRU Epoch 4/5, Batch 2100, Loss: 0.9953\n",
      "GRU Epoch 4/5, Batch 2200, Loss: 0.9541\n",
      "GRU Epoch 4/5, Batch 2300, Loss: 0.9444\n",
      "GRU Epoch 4/5, Batch 2400, Loss: 0.9448\n",
      "GRU Epoch 4/5, Batch 2500, Loss: 0.9073\n",
      "GRU Epoch 4/5, Batch 2600, Loss: 0.9307\n",
      "GRU Epoch 4 average loss: 1.0438\n",
      "GRU Epoch 5/5, Batch 0, Loss: 0.8911\n",
      "GRU Epoch 5/5, Batch 100, Loss: 0.8507\n",
      "GRU Epoch 5/5, Batch 200, Loss: 0.8194\n",
      "GRU Epoch 5/5, Batch 300, Loss: 0.8599\n",
      "GRU Epoch 5/5, Batch 400, Loss: 0.8494\n",
      "GRU Epoch 5/5, Batch 500, Loss: 0.8257\n",
      "GRU Epoch 5/5, Batch 600, Loss: 0.8181\n",
      "GRU Epoch 5/5, Batch 700, Loss: 0.8106\n",
      "GRU Epoch 5/5, Batch 800, Loss: 0.7972\n",
      "GRU Epoch 5/5, Batch 900, Loss: 0.8466\n",
      "GRU Epoch 5/5, Batch 1000, Loss: 0.7495\n",
      "GRU Epoch 5/5, Batch 1100, Loss: 0.7681\n",
      "GRU Epoch 5/5, Batch 1200, Loss: 0.8333\n",
      "GRU Epoch 5/5, Batch 1300, Loss: 0.7873\n",
      "GRU Epoch 5/5, Batch 1400, Loss: 0.7629\n",
      "GRU Epoch 5/5, Batch 1500, Loss: 0.8390\n",
      "GRU Epoch 5/5, Batch 1600, Loss: 0.7183\n",
      "GRU Epoch 5/5, Batch 1700, Loss: 0.7419\n",
      "GRU Epoch 5/5, Batch 1800, Loss: 0.7890\n",
      "GRU Epoch 5/5, Batch 1900, Loss: 0.7294\n",
      "GRU Epoch 5/5, Batch 2000, Loss: 0.8113\n",
      "GRU Epoch 5/5, Batch 2100, Loss: 0.7490\n",
      "GRU Epoch 5/5, Batch 2200, Loss: 0.8120\n",
      "GRU Epoch 5/5, Batch 2300, Loss: 0.7804\n",
      "GRU Epoch 5/5, Batch 2400, Loss: 0.7211\n",
      "GRU Epoch 5/5, Batch 2500, Loss: 0.7770\n",
      "GRU Epoch 5/5, Batch 2600, Loss: 0.7218\n",
      "GRU Epoch 5 average loss: 0.7987\n"
     ]
    }
   ],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embed = self.embedding(x)\n",
    "        output, hidden = self.gru(embed, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
    "\n",
    "model_gru = GRUModel(vocab_size, embedding_dim, hidden_size, num_layers).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model_gru.parameters(), lr=0.001)\n",
    "num_epochs = 5\n",
    "\n",
    "model_gru.train()\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0.0\n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        current_batch_size = inputs.size(0)\n",
    "        hidden = model_gru.init_hidden(current_batch_size).to(device)\n",
    "        outputs, hidden = model_gru(inputs, hidden)\n",
    "        outputs = outputs.reshape(-1, vocab_size)\n",
    "        targets = targets.reshape(-1)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"GRU Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"GRU Epoch {epoch+1} average loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Сгенерированный текст GRU:\n",
      " Alice was beginning to get very tired that could really be feeling reasonable he would be so simple since his father 's marriage talk as possible undoubtedly man has notice her father and he seemed to have\n"
     ]
    }
   ],
   "source": [
    "def generate_text_gru(model, start_text, vocab, inv_vocab, device, gen_length=10):\n",
    "    model.eval()\n",
    "    tokens = word_tokenize(start_text)\n",
    "    input_indices = [vocab.get(token, 0) for token in tokens]\n",
    "    input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(input_tensor.size(0)).to(device)\n",
    "    generated_tokens = tokens.copy()\n",
    "    for _ in range(gen_length):\n",
    "        outputs, hidden = model(input_tensor, hidden)\n",
    "        logits = outputs[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        generated_tokens.append(inv_vocab[next_token_idx])\n",
    "        input_tensor = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
    "    return \" \".join(generated_tokens)\n",
    "\n",
    "generated_gru = generate_text_gru(model_gru, \"Alice was beginning to get very tired\", vocab, inv_vocab, device, gen_length=30)\n",
    "print(\"Сгенерированный текст GRU:\\n\", generated_gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
