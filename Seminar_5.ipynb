{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmHONUs_fgfy"
      },
      "source": [
        "Beam-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI3nh2mSf7y2"
      },
      "source": [
        "- https://habr.com/ru/articles/599673/\n",
        "- https://habr.com/ru/articles/745314/\n",
        "- https://habr.com/ru/articles/346578/\n",
        "- https://habr.com/ru/companies/vk/articles/579412/\n",
        "- https://habr.com/ru/companies/sberdevices/articles/666420/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9uDoCm7hp-S"
      },
      "source": [
        "Beam Search — это эвристический алгоритм поиска, широко применяемый в задачах обработки естественного языка (Natural Language Processing, NLP). Основная цель этого метода заключается в поиске оптимальной последовательности токенов (слов или символов) в условиях огромного количества возможных вариантов. Давайте рассмотрим подробнее, как Beam Search применяется в контексте NLP.\n",
        "Что такое Beam Search?\n",
        "\n",
        "Beam Search — это способ поиска, основанный на эвристическом подходе, который пытается найти лучшую последовательность элементов (например, слов в предложении) путём рассмотрения ограниченного числа возможных путей (или \"лучей\") на каждом этапе. Идея состоит в том, чтобы сосредоточиться лишь на небольшом количестве наиболее перспективных вариантов, игнорируя остальные. Этот метод существенно сокращает пространство поиска, делая возможным эффективное нахождение хороших решений в ситуациях, когда полное переборное исследование всех возможных вариантов невозможно.\n",
        "Как работает Beam Search в NLP?\n",
        "\n",
        "Представим себе задачу автоматического формирования текста или машинного перевода. Задача состоит в том, чтобы на основе некоторого начального ввода (например, \"начало предложения\") найти наилучшую последовательность слов, образующих осмысленное предложение. Рассмотрим пошагово, как это происходит с применением Beam Search:\n",
        "\n",
        "- Инициализация: Начинаем с начального состояния (например, пустого ввода или первого слова).\n",
        "- Генерация гипотез: На каждом шаге мы генерируем возможные продолжения для текущего состояния. В случае языковой модели это могут быть следующие слова, которые могут быть использованы в следующем токене.\n",
        "- Оценка гипотез: Каждое возможное продолжение оценивается по некоторой метрике (например, по вероятности того, что именно это слово должно идти дальше согласно модели).\n",
        "- Выбор лучших гипотез: Из всех возможных продолжений выбирается ограниченное количество лучших (обычно это параметр beam_width, задающий ширину пучка). Остальные гипотезы отбрасываются.\n",
        "- Рекурсия: Процесс повторяется для каждого выбранного продолжения до тех пор, пока не будет достигнут конец предложения или другая заранее определённая точка остановки.\n",
        "- Окончание: Когда поиск завершён, возвращаются лучшие последовательности, найденные методом Beam Search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q28YtTIgiQJD"
      },
      "source": [
        "Плюсы и минусы Beam Search в NLP\n",
        "Плюсы:\n",
        "\n",
        "- Эффективность: Позволяет находить хорошие решения в больших пространствах состояний без полного перебора.\n",
        "- Контролируемая сложность: Параметр beam_width даёт возможность настраивать компромисс между точностью и производительностью.\n",
        "- Подходит для сложных задач: Используется в широком спектре приложений, включая машинный перевод, автоматическое формулирование текста и др.\n",
        "\n",
        "Минусы:\n",
        "\n",
        "- Не гарантирует оптимальное решение: Может пропустить лучшие варианты, если они находятся вне текущего пучка.\n",
        "- Чувствителен к параметрам: Неправильно подобранная ширина пучка может привести либо к пропуску хороших решений, либо к чрезмерному увеличению времени вычисления.\n",
        "- Требует настройки: Подбор правильных параметров (ширина пучка, длина последовательности и т.п.) требует экспериментов и тестов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import string\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLHy37J4hv_2"
      },
      "source": [
        "## Задача\n",
        "1. Собрать полный пайплан по генерации текста с примением процедуры препроцессинга, токенизации и генерации используя алгоритм beam search. Попытаться добитьсякачественной генерации текста. За основу взять лабораторную №3. Попытаться реализовать как можно болеьше предложенных методов.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": [
        "print(gutenberg.fileids())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Размер словаря: 52494\n"
          ]
        }
      ],
      "source": [
        "file_ids = gutenberg.fileids()\n",
        "texts_tokens = {} \n",
        "\n",
        "for fileid in file_ids:\n",
        "    text = gutenberg.raw(fileid)\n",
        "    tokens = [token.lower() for token in word_tokenize(text) if token not in string.punctuation]\n",
        "    texts_tokens[fileid] = tokens\n",
        "\n",
        "all_tokens = [token for tokens in texts_tokens.values() for token in tokens]\n",
        "vocab = {token: idx for idx, token in enumerate(sorted(set(all_tokens)))}\n",
        "\n",
        "print(\"Размер словаря:\", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[18621, 11210, 27723, 8239, 1914, 50340, 25779, 12151, 25779, 18621, 51871, 23995, 12903, 7047, 39668, 51719, 5517, 13357, 25209, 7047]\n"
          ]
        }
      ],
      "source": [
        "texts_indices = {}\n",
        "for fileid, tokens in texts_tokens.items():\n",
        "    indices = [vocab[token] for token in tokens]\n",
        "    texts_indices[fileid] = indices\n",
        "\n",
        "print(texts_indices[file_ids[0]][:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_file = list(texts_indices.keys())[0]\n",
        "data = texts_indices[sample_file]\n",
        "seq_length = 30\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, seq_length):\n",
        "        self.data = data\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx:idx+self.seq_length]\n",
        "        y = self.data[idx+1: idx+self.seq_length+1]\n",
        "        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "dataset = TextDataset(data, seq_length)\n",
        "batch_size = 64\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LSTM Epoch 1/20, Batch 0, Loss: 10.8706\n",
            "LSTM Epoch 1/20, Batch 100, Loss: 6.3852\n",
            "LSTM Epoch 1/20, Batch 200, Loss: 6.2832\n",
            "LSTM Epoch 1/20, Batch 300, Loss: 6.3258\n",
            "LSTM Epoch 1/20, Batch 400, Loss: 6.2900\n",
            "LSTM Epoch 1/20, Batch 500, Loss: 6.1726\n",
            "LSTM Epoch 1/20, Batch 600, Loss: 6.0906\n",
            "LSTM Epoch 1/20, Batch 700, Loss: 6.0129\n",
            "LSTM Epoch 1/20, Batch 800, Loss: 6.0186\n",
            "LSTM Epoch 1/20, Batch 900, Loss: 5.9063\n",
            "LSTM Epoch 1/20, Batch 1000, Loss: 5.8071\n",
            "LSTM Epoch 1/20, Batch 1100, Loss: 5.7049\n",
            "LSTM Epoch 1/20, Batch 1200, Loss: 5.6198\n",
            "LSTM Epoch 1/20, Batch 1300, Loss: 5.8166\n",
            "LSTM Epoch 1/20, Batch 1400, Loss: 5.5659\n",
            "LSTM Epoch 1/20, Batch 1500, Loss: 5.4893\n",
            "LSTM Epoch 1/20, Batch 1600, Loss: 5.4529\n",
            "LSTM Epoch 1/20, Batch 1700, Loss: 5.3597\n",
            "LSTM Epoch 1/20, Batch 1800, Loss: 5.4809\n",
            "LSTM Epoch 1/20, Batch 1900, Loss: 5.3500\n",
            "LSTM Epoch 1/20, Batch 2000, Loss: 5.3125\n",
            "LSTM Epoch 1/20, Batch 2100, Loss: 5.3245\n",
            "LSTM Epoch 1/20, Batch 2200, Loss: 5.3821\n",
            "LSTM Epoch 1/20, Batch 2300, Loss: 5.1215\n",
            "LSTM Epoch 1/20, Batch 2400, Loss: 5.2818\n",
            "LSTM Epoch 1/20, Batch 2500, Loss: 5.1010\n",
            "LSTM Epoch 1/20, Batch 2600, Loss: 5.0414\n",
            "LSTM Epoch 1 average loss: 5.7224\n",
            "LSTM Epoch 2/20, Batch 0, Loss: 5.1668\n",
            "LSTM Epoch 2/20, Batch 100, Loss: 5.0750\n",
            "LSTM Epoch 2/20, Batch 200, Loss: 4.9814\n",
            "LSTM Epoch 2/20, Batch 300, Loss: 4.9457\n",
            "LSTM Epoch 2/20, Batch 400, Loss: 4.8890\n",
            "LSTM Epoch 2/20, Batch 500, Loss: 4.8611\n",
            "LSTM Epoch 2/20, Batch 600, Loss: 4.8854\n",
            "LSTM Epoch 2/20, Batch 700, Loss: 4.8173\n",
            "LSTM Epoch 2/20, Batch 800, Loss: 4.8181\n",
            "LSTM Epoch 2/20, Batch 900, Loss: 4.7720\n",
            "LSTM Epoch 2/20, Batch 1000, Loss: 4.6813\n",
            "LSTM Epoch 2/20, Batch 1100, Loss: 4.7061\n",
            "LSTM Epoch 2/20, Batch 1200, Loss: 4.6878\n",
            "LSTM Epoch 2/20, Batch 1300, Loss: 4.6628\n",
            "LSTM Epoch 2/20, Batch 1400, Loss: 4.6037\n",
            "LSTM Epoch 2/20, Batch 1500, Loss: 4.5969\n",
            "LSTM Epoch 2/20, Batch 1600, Loss: 4.5827\n",
            "LSTM Epoch 2/20, Batch 1700, Loss: 4.6242\n",
            "LSTM Epoch 2/20, Batch 1800, Loss: 4.5354\n",
            "LSTM Epoch 2/20, Batch 1900, Loss: 4.4838\n",
            "LSTM Epoch 2/20, Batch 2000, Loss: 4.5437\n",
            "LSTM Epoch 2/20, Batch 2100, Loss: 4.4040\n",
            "LSTM Epoch 2/20, Batch 2200, Loss: 4.3441\n",
            "LSTM Epoch 2/20, Batch 2300, Loss: 4.3561\n",
            "LSTM Epoch 2/20, Batch 2400, Loss: 4.2339\n",
            "LSTM Epoch 2/20, Batch 2500, Loss: 4.3087\n",
            "LSTM Epoch 2/20, Batch 2600, Loss: 4.2382\n",
            "LSTM Epoch 2 average loss: 4.6523\n",
            "LSTM Epoch 3/20, Batch 0, Loss: 4.1383\n",
            "LSTM Epoch 3/20, Batch 100, Loss: 4.1620\n",
            "LSTM Epoch 3/20, Batch 200, Loss: 4.1168\n",
            "LSTM Epoch 3/20, Batch 300, Loss: 4.1901\n",
            "LSTM Epoch 3/20, Batch 400, Loss: 4.1257\n",
            "LSTM Epoch 3/20, Batch 500, Loss: 4.1488\n",
            "LSTM Epoch 3/20, Batch 600, Loss: 4.0368\n",
            "LSTM Epoch 3/20, Batch 700, Loss: 3.9435\n",
            "LSTM Epoch 3/20, Batch 800, Loss: 3.9146\n",
            "LSTM Epoch 3/20, Batch 900, Loss: 3.9567\n",
            "LSTM Epoch 3/20, Batch 1000, Loss: 3.9643\n",
            "LSTM Epoch 3/20, Batch 1100, Loss: 3.7625\n",
            "LSTM Epoch 3/20, Batch 1200, Loss: 3.8003\n",
            "LSTM Epoch 3/20, Batch 1300, Loss: 3.7732\n",
            "LSTM Epoch 3/20, Batch 1400, Loss: 3.7028\n",
            "LSTM Epoch 3/20, Batch 1500, Loss: 3.7934\n",
            "LSTM Epoch 3/20, Batch 1600, Loss: 3.6636\n",
            "LSTM Epoch 3/20, Batch 1700, Loss: 3.6333\n",
            "LSTM Epoch 3/20, Batch 1800, Loss: 3.5895\n",
            "LSTM Epoch 3/20, Batch 1900, Loss: 3.5399\n",
            "LSTM Epoch 3/20, Batch 2000, Loss: 3.4824\n",
            "LSTM Epoch 3/20, Batch 2100, Loss: 3.4670\n",
            "LSTM Epoch 3/20, Batch 2200, Loss: 3.3932\n",
            "LSTM Epoch 3/20, Batch 2300, Loss: 3.4034\n",
            "LSTM Epoch 3/20, Batch 2400, Loss: 3.3370\n",
            "LSTM Epoch 3/20, Batch 2500, Loss: 3.2683\n",
            "LSTM Epoch 3/20, Batch 2600, Loss: 3.2029\n",
            "LSTM Epoch 3 average loss: 3.7463\n",
            "LSTM Epoch 4/20, Batch 0, Loss: 3.2234\n",
            "LSTM Epoch 4/20, Batch 100, Loss: 3.2097\n",
            "LSTM Epoch 4/20, Batch 200, Loss: 3.1404\n",
            "LSTM Epoch 4/20, Batch 300, Loss: 3.2219\n",
            "LSTM Epoch 4/20, Batch 400, Loss: 3.1601\n",
            "LSTM Epoch 4/20, Batch 500, Loss: 3.0253\n",
            "LSTM Epoch 4/20, Batch 600, Loss: 3.0063\n",
            "LSTM Epoch 4/20, Batch 700, Loss: 3.0114\n",
            "LSTM Epoch 4/20, Batch 800, Loss: 2.9725\n",
            "LSTM Epoch 4/20, Batch 900, Loss: 2.9647\n",
            "LSTM Epoch 4/20, Batch 1000, Loss: 2.7405\n",
            "LSTM Epoch 4/20, Batch 1100, Loss: 2.8507\n",
            "LSTM Epoch 4/20, Batch 1200, Loss: 2.7883\n",
            "LSTM Epoch 4/20, Batch 1300, Loss: 2.7271\n",
            "LSTM Epoch 4/20, Batch 1400, Loss: 2.7482\n",
            "LSTM Epoch 4/20, Batch 1500, Loss: 2.6746\n",
            "LSTM Epoch 4/20, Batch 1600, Loss: 2.6366\n",
            "LSTM Epoch 4/20, Batch 1700, Loss: 2.6193\n",
            "LSTM Epoch 4/20, Batch 1800, Loss: 2.5963\n",
            "LSTM Epoch 4/20, Batch 1900, Loss: 2.5744\n",
            "LSTM Epoch 4/20, Batch 2000, Loss: 2.6381\n",
            "LSTM Epoch 4/20, Batch 2100, Loss: 2.5453\n",
            "LSTM Epoch 4/20, Batch 2200, Loss: 2.5594\n",
            "LSTM Epoch 4/20, Batch 2300, Loss: 2.4999\n",
            "LSTM Epoch 4/20, Batch 2400, Loss: 2.3730\n",
            "LSTM Epoch 4/20, Batch 2500, Loss: 2.4126\n",
            "LSTM Epoch 4/20, Batch 2600, Loss: 2.4042\n",
            "LSTM Epoch 4 average loss: 2.7582\n",
            "LSTM Epoch 5/20, Batch 0, Loss: 2.2936\n",
            "LSTM Epoch 5/20, Batch 100, Loss: 2.2477\n",
            "LSTM Epoch 5/20, Batch 200, Loss: 2.1894\n",
            "LSTM Epoch 5/20, Batch 300, Loss: 2.2344\n",
            "LSTM Epoch 5/20, Batch 400, Loss: 2.2686\n",
            "LSTM Epoch 5/20, Batch 500, Loss: 2.2242\n",
            "LSTM Epoch 5/20, Batch 600, Loss: 2.2362\n",
            "LSTM Epoch 5/20, Batch 700, Loss: 2.1293\n",
            "LSTM Epoch 5/20, Batch 800, Loss: 2.1027\n",
            "LSTM Epoch 5/20, Batch 900, Loss: 2.1375\n",
            "LSTM Epoch 5/20, Batch 1000, Loss: 2.1319\n",
            "LSTM Epoch 5/20, Batch 1100, Loss: 2.0837\n",
            "LSTM Epoch 5/20, Batch 1200, Loss: 2.0162\n",
            "LSTM Epoch 5/20, Batch 1300, Loss: 2.0006\n",
            "LSTM Epoch 5/20, Batch 1400, Loss: 2.0528\n",
            "LSTM Epoch 5/20, Batch 1500, Loss: 1.9852\n",
            "LSTM Epoch 5/20, Batch 1600, Loss: 1.8429\n",
            "LSTM Epoch 5/20, Batch 1700, Loss: 1.8651\n",
            "LSTM Epoch 5/20, Batch 1800, Loss: 1.8974\n",
            "LSTM Epoch 5/20, Batch 1900, Loss: 1.9286\n",
            "LSTM Epoch 5/20, Batch 2000, Loss: 1.8616\n",
            "LSTM Epoch 5/20, Batch 2100, Loss: 1.8703\n",
            "LSTM Epoch 5/20, Batch 2200, Loss: 1.7847\n",
            "LSTM Epoch 5/20, Batch 2300, Loss: 1.8143\n",
            "LSTM Epoch 5/20, Batch 2400, Loss: 1.8696\n",
            "LSTM Epoch 5/20, Batch 2500, Loss: 1.7583\n",
            "LSTM Epoch 5/20, Batch 2600, Loss: 1.7062\n",
            "LSTM Epoch 5 average loss: 2.0161\n",
            "LSTM Epoch 6/20, Batch 0, Loss: 1.7417\n",
            "LSTM Epoch 6/20, Batch 100, Loss: 1.6331\n",
            "LSTM Epoch 6/20, Batch 200, Loss: 1.6539\n",
            "LSTM Epoch 6/20, Batch 300, Loss: 1.7028\n",
            "LSTM Epoch 6/20, Batch 400, Loss: 1.7377\n",
            "LSTM Epoch 6/20, Batch 500, Loss: 1.6232\n",
            "LSTM Epoch 6/20, Batch 600, Loss: 1.5392\n",
            "LSTM Epoch 6/20, Batch 700, Loss: 1.6239\n",
            "LSTM Epoch 6/20, Batch 800, Loss: 1.6343\n",
            "LSTM Epoch 6/20, Batch 900, Loss: 1.5046\n",
            "LSTM Epoch 6/20, Batch 1000, Loss: 1.4978\n",
            "LSTM Epoch 6/20, Batch 1100, Loss: 1.4646\n",
            "LSTM Epoch 6/20, Batch 1200, Loss: 1.5932\n",
            "LSTM Epoch 6/20, Batch 1300, Loss: 1.4918\n",
            "LSTM Epoch 6/20, Batch 1400, Loss: 1.4941\n",
            "LSTM Epoch 6/20, Batch 1500, Loss: 1.4862\n",
            "LSTM Epoch 6/20, Batch 1600, Loss: 1.4908\n",
            "LSTM Epoch 6/20, Batch 1700, Loss: 1.4887\n",
            "LSTM Epoch 6/20, Batch 1800, Loss: 1.4854\n",
            "LSTM Epoch 6/20, Batch 1900, Loss: 1.4041\n",
            "LSTM Epoch 6/20, Batch 2000, Loss: 1.4455\n",
            "LSTM Epoch 6/20, Batch 2100, Loss: 1.3814\n",
            "LSTM Epoch 6/20, Batch 2200, Loss: 1.3646\n",
            "LSTM Epoch 6/20, Batch 2300, Loss: 1.4259\n",
            "LSTM Epoch 6/20, Batch 2400, Loss: 1.3125\n",
            "LSTM Epoch 6/20, Batch 2500, Loss: 1.2873\n",
            "LSTM Epoch 6/20, Batch 2600, Loss: 1.3730\n",
            "LSTM Epoch 6 average loss: 1.4924\n",
            "LSTM Epoch 7/20, Batch 0, Loss: 1.2708\n",
            "LSTM Epoch 7/20, Batch 100, Loss: 1.2290\n",
            "LSTM Epoch 7/20, Batch 200, Loss: 1.2322\n",
            "LSTM Epoch 7/20, Batch 300, Loss: 1.3197\n",
            "LSTM Epoch 7/20, Batch 400, Loss: 1.2296\n",
            "LSTM Epoch 7/20, Batch 500, Loss: 1.2189\n",
            "LSTM Epoch 7/20, Batch 600, Loss: 1.2097\n",
            "LSTM Epoch 7/20, Batch 700, Loss: 1.1227\n",
            "LSTM Epoch 7/20, Batch 800, Loss: 1.2400\n",
            "LSTM Epoch 7/20, Batch 900, Loss: 1.1672\n",
            "LSTM Epoch 7/20, Batch 1000, Loss: 1.1913\n",
            "LSTM Epoch 7/20, Batch 1100, Loss: 1.1346\n",
            "LSTM Epoch 7/20, Batch 1200, Loss: 1.1917\n",
            "LSTM Epoch 7/20, Batch 1300, Loss: 1.1337\n",
            "LSTM Epoch 7/20, Batch 1400, Loss: 1.1167\n",
            "LSTM Epoch 7/20, Batch 1500, Loss: 1.1763\n",
            "LSTM Epoch 7/20, Batch 1600, Loss: 1.0667\n",
            "LSTM Epoch 7/20, Batch 1700, Loss: 1.1484\n",
            "LSTM Epoch 7/20, Batch 1800, Loss: 1.0996\n",
            "LSTM Epoch 7/20, Batch 1900, Loss: 1.0253\n",
            "LSTM Epoch 7/20, Batch 2000, Loss: 1.0145\n",
            "LSTM Epoch 7/20, Batch 2100, Loss: 1.0662\n",
            "LSTM Epoch 7/20, Batch 2200, Loss: 1.0493\n",
            "LSTM Epoch 7/20, Batch 2300, Loss: 1.0576\n",
            "LSTM Epoch 7/20, Batch 2400, Loss: 1.0985\n",
            "LSTM Epoch 7/20, Batch 2500, Loss: 0.9728\n",
            "LSTM Epoch 7/20, Batch 2600, Loss: 1.0844\n",
            "LSTM Epoch 7 average loss: 1.1278\n",
            "LSTM Epoch 8/20, Batch 0, Loss: 0.9239\n",
            "LSTM Epoch 8/20, Batch 100, Loss: 0.9393\n",
            "LSTM Epoch 8/20, Batch 200, Loss: 0.9409\n",
            "LSTM Epoch 8/20, Batch 300, Loss: 0.9295\n",
            "LSTM Epoch 8/20, Batch 400, Loss: 0.9076\n",
            "LSTM Epoch 8/20, Batch 500, Loss: 0.9230\n",
            "LSTM Epoch 8/20, Batch 600, Loss: 0.9196\n",
            "LSTM Epoch 8/20, Batch 700, Loss: 0.9566\n",
            "LSTM Epoch 8/20, Batch 800, Loss: 0.9255\n",
            "LSTM Epoch 8/20, Batch 900, Loss: 0.8980\n",
            "LSTM Epoch 8/20, Batch 1000, Loss: 0.9086\n",
            "LSTM Epoch 8/20, Batch 1100, Loss: 0.9607\n",
            "LSTM Epoch 8/20, Batch 1200, Loss: 0.8823\n",
            "LSTM Epoch 8/20, Batch 1300, Loss: 0.8513\n",
            "LSTM Epoch 8/20, Batch 1400, Loss: 0.8462\n",
            "LSTM Epoch 8/20, Batch 1500, Loss: 0.8984\n",
            "LSTM Epoch 8/20, Batch 1600, Loss: 0.8778\n",
            "LSTM Epoch 8/20, Batch 1700, Loss: 0.8492\n",
            "LSTM Epoch 8/20, Batch 1800, Loss: 0.8333\n",
            "LSTM Epoch 8/20, Batch 1900, Loss: 0.8597\n",
            "LSTM Epoch 8/20, Batch 2000, Loss: 0.8206\n",
            "LSTM Epoch 8/20, Batch 2100, Loss: 0.8368\n",
            "LSTM Epoch 8/20, Batch 2200, Loss: 0.7994\n",
            "LSTM Epoch 8/20, Batch 2300, Loss: 0.7926\n",
            "LSTM Epoch 8/20, Batch 2400, Loss: 0.8123\n",
            "LSTM Epoch 8/20, Batch 2500, Loss: 0.8879\n",
            "LSTM Epoch 8/20, Batch 2600, Loss: 0.8030\n",
            "LSTM Epoch 8 average loss: 0.8817\n",
            "LSTM Epoch 9/20, Batch 0, Loss: 0.7592\n",
            "LSTM Epoch 9/20, Batch 100, Loss: 0.7529\n",
            "LSTM Epoch 9/20, Batch 200, Loss: 0.7330\n",
            "LSTM Epoch 9/20, Batch 300, Loss: 0.7306\n",
            "LSTM Epoch 9/20, Batch 400, Loss: 0.7548\n",
            "LSTM Epoch 9/20, Batch 500, Loss: 0.7166\n",
            "LSTM Epoch 9/20, Batch 600, Loss: 0.7816\n",
            "LSTM Epoch 9/20, Batch 700, Loss: 0.7071\n",
            "LSTM Epoch 9/20, Batch 800, Loss: 0.7526\n",
            "LSTM Epoch 9/20, Batch 900, Loss: 0.7653\n",
            "LSTM Epoch 9/20, Batch 1000, Loss: 0.7533\n",
            "LSTM Epoch 9/20, Batch 1100, Loss: 0.7360\n",
            "LSTM Epoch 9/20, Batch 1200, Loss: 0.7740\n",
            "LSTM Epoch 9/20, Batch 1300, Loss: 0.7496\n",
            "LSTM Epoch 9/20, Batch 1400, Loss: 0.7477\n",
            "LSTM Epoch 9/20, Batch 1500, Loss: 0.7095\n",
            "LSTM Epoch 9/20, Batch 1600, Loss: 0.7276\n",
            "LSTM Epoch 9/20, Batch 1700, Loss: 0.7095\n",
            "LSTM Epoch 9/20, Batch 1800, Loss: 0.7450\n",
            "LSTM Epoch 9/20, Batch 1900, Loss: 0.7036\n",
            "LSTM Epoch 9/20, Batch 2000, Loss: 0.7577\n",
            "LSTM Epoch 9/20, Batch 2100, Loss: 0.7466\n",
            "LSTM Epoch 9/20, Batch 2200, Loss: 0.6926\n",
            "LSTM Epoch 9/20, Batch 2300, Loss: 0.6983\n",
            "LSTM Epoch 9/20, Batch 2400, Loss: 0.6913\n",
            "LSTM Epoch 9/20, Batch 2500, Loss: 0.7140\n",
            "LSTM Epoch 9/20, Batch 2600, Loss: 0.6695\n",
            "LSTM Epoch 9 average loss: 0.7250\n",
            "LSTM Epoch 10/20, Batch 0, Loss: 0.6062\n",
            "LSTM Epoch 10/20, Batch 100, Loss: 0.6551\n",
            "LSTM Epoch 10/20, Batch 200, Loss: 0.6584\n",
            "LSTM Epoch 10/20, Batch 300, Loss: 0.6371\n",
            "LSTM Epoch 10/20, Batch 400, Loss: 0.6045\n",
            "LSTM Epoch 10/20, Batch 500, Loss: 0.6066\n",
            "LSTM Epoch 10/20, Batch 600, Loss: 0.5993\n",
            "LSTM Epoch 10/20, Batch 700, Loss: 0.6122\n",
            "LSTM Epoch 10/20, Batch 800, Loss: 0.6732\n",
            "LSTM Epoch 10/20, Batch 900, Loss: 0.6424\n",
            "LSTM Epoch 10/20, Batch 1000, Loss: 0.6030\n",
            "LSTM Epoch 10/20, Batch 1100, Loss: 0.6092\n",
            "LSTM Epoch 10/20, Batch 1200, Loss: 0.6545\n",
            "LSTM Epoch 10/20, Batch 1300, Loss: 0.6680\n",
            "LSTM Epoch 10/20, Batch 1400, Loss: 0.6432\n",
            "LSTM Epoch 10/20, Batch 1500, Loss: 0.6377\n",
            "LSTM Epoch 10/20, Batch 1600, Loss: 0.5841\n",
            "LSTM Epoch 10/20, Batch 1700, Loss: 0.6504\n",
            "LSTM Epoch 10/20, Batch 1800, Loss: 0.6145\n",
            "LSTM Epoch 10/20, Batch 1900, Loss: 0.6204\n",
            "LSTM Epoch 10/20, Batch 2000, Loss: 0.6327\n",
            "LSTM Epoch 10/20, Batch 2100, Loss: 0.5728\n",
            "LSTM Epoch 10/20, Batch 2200, Loss: 0.5728\n",
            "LSTM Epoch 10/20, Batch 2300, Loss: 0.6021\n",
            "LSTM Epoch 10/20, Batch 2400, Loss: 0.6310\n",
            "LSTM Epoch 10/20, Batch 2500, Loss: 0.6269\n",
            "LSTM Epoch 10/20, Batch 2600, Loss: 0.6415\n",
            "LSTM Epoch 10 average loss: 0.6285\n",
            "LSTM Epoch 11/20, Batch 0, Loss: 0.5582\n",
            "LSTM Epoch 11/20, Batch 100, Loss: 0.6267\n",
            "LSTM Epoch 11/20, Batch 200, Loss: 0.5583\n",
            "LSTM Epoch 11/20, Batch 300, Loss: 0.5890\n",
            "LSTM Epoch 11/20, Batch 400, Loss: 0.5706\n",
            "LSTM Epoch 11/20, Batch 500, Loss: 0.5649\n",
            "LSTM Epoch 11/20, Batch 600, Loss: 0.5654\n",
            "LSTM Epoch 11/20, Batch 700, Loss: 0.5929\n",
            "LSTM Epoch 11/20, Batch 800, Loss: 0.5201\n",
            "LSTM Epoch 11/20, Batch 900, Loss: 0.5722\n",
            "LSTM Epoch 11/20, Batch 1000, Loss: 0.5456\n",
            "LSTM Epoch 11/20, Batch 1100, Loss: 0.5771\n",
            "LSTM Epoch 11/20, Batch 1200, Loss: 0.5848\n",
            "LSTM Epoch 11/20, Batch 1300, Loss: 0.5832\n",
            "LSTM Epoch 11/20, Batch 1400, Loss: 0.5554\n",
            "LSTM Epoch 11/20, Batch 1500, Loss: 0.5856\n",
            "LSTM Epoch 11/20, Batch 1600, Loss: 0.5476\n",
            "LSTM Epoch 11/20, Batch 1700, Loss: 0.5696\n",
            "LSTM Epoch 11/20, Batch 1800, Loss: 0.5463\n",
            "LSTM Epoch 11/20, Batch 1900, Loss: 0.5842\n",
            "LSTM Epoch 11/20, Batch 2000, Loss: 0.5742\n",
            "LSTM Epoch 11/20, Batch 2100, Loss: 0.5578\n",
            "LSTM Epoch 11/20, Batch 2200, Loss: 0.5405\n",
            "LSTM Epoch 11/20, Batch 2300, Loss: 0.5714\n",
            "LSTM Epoch 11/20, Batch 2400, Loss: 0.5868\n",
            "LSTM Epoch 11/20, Batch 2500, Loss: 0.5977\n",
            "LSTM Epoch 11/20, Batch 2600, Loss: 0.5646\n",
            "LSTM Epoch 11 average loss: 0.5676\n",
            "LSTM Epoch 12/20, Batch 0, Loss: 0.5250\n",
            "LSTM Epoch 12/20, Batch 100, Loss: 0.5097\n",
            "LSTM Epoch 12/20, Batch 200, Loss: 0.5156\n",
            "LSTM Epoch 12/20, Batch 300, Loss: 0.5148\n",
            "LSTM Epoch 12/20, Batch 400, Loss: 0.5485\n",
            "LSTM Epoch 12/20, Batch 500, Loss: 0.5465\n",
            "LSTM Epoch 12/20, Batch 600, Loss: 0.5228\n",
            "LSTM Epoch 12/20, Batch 700, Loss: 0.5679\n",
            "LSTM Epoch 12/20, Batch 800, Loss: 0.5166\n",
            "LSTM Epoch 12/20, Batch 900, Loss: 0.5268\n",
            "LSTM Epoch 12/20, Batch 1000, Loss: 0.5025\n",
            "LSTM Epoch 12/20, Batch 1100, Loss: 0.5449\n",
            "LSTM Epoch 12/20, Batch 1200, Loss: 0.5076\n",
            "LSTM Epoch 12/20, Batch 1300, Loss: 0.5182\n",
            "LSTM Epoch 12/20, Batch 1400, Loss: 0.5368\n",
            "LSTM Epoch 12/20, Batch 1500, Loss: 0.5069\n",
            "LSTM Epoch 12/20, Batch 1600, Loss: 0.5317\n",
            "LSTM Epoch 12/20, Batch 1700, Loss: 0.5420\n",
            "LSTM Epoch 12/20, Batch 1800, Loss: 0.5164\n",
            "LSTM Epoch 12/20, Batch 1900, Loss: 0.5401\n",
            "LSTM Epoch 12/20, Batch 2000, Loss: 0.5030\n",
            "LSTM Epoch 12/20, Batch 2100, Loss: 0.5226\n",
            "LSTM Epoch 12/20, Batch 2200, Loss: 0.5370\n",
            "LSTM Epoch 12/20, Batch 2300, Loss: 0.5306\n",
            "LSTM Epoch 12/20, Batch 2400, Loss: 0.5305\n",
            "LSTM Epoch 12/20, Batch 2500, Loss: 0.5342\n",
            "LSTM Epoch 12/20, Batch 2600, Loss: 0.5160\n",
            "LSTM Epoch 12 average loss: 0.5264\n",
            "LSTM Epoch 13/20, Batch 0, Loss: 0.5044\n",
            "LSTM Epoch 13/20, Batch 100, Loss: 0.4517\n",
            "LSTM Epoch 13/20, Batch 200, Loss: 0.5073\n",
            "LSTM Epoch 13/20, Batch 300, Loss: 0.5178\n",
            "LSTM Epoch 13/20, Batch 400, Loss: 0.5155\n",
            "LSTM Epoch 13/20, Batch 500, Loss: 0.4765\n",
            "LSTM Epoch 13/20, Batch 600, Loss: 0.5043\n",
            "LSTM Epoch 13/20, Batch 700, Loss: 0.5005\n",
            "LSTM Epoch 13/20, Batch 800, Loss: 0.4979\n",
            "LSTM Epoch 13/20, Batch 900, Loss: 0.4827\n",
            "LSTM Epoch 13/20, Batch 1000, Loss: 0.4833\n",
            "LSTM Epoch 13/20, Batch 1100, Loss: 0.5013\n",
            "LSTM Epoch 13/20, Batch 1200, Loss: 0.4760\n",
            "LSTM Epoch 13/20, Batch 1300, Loss: 0.4937\n",
            "LSTM Epoch 13/20, Batch 1400, Loss: 0.4941\n",
            "LSTM Epoch 13/20, Batch 1500, Loss: 0.4856\n",
            "LSTM Epoch 13/20, Batch 1600, Loss: 0.5191\n",
            "LSTM Epoch 13/20, Batch 1700, Loss: 0.5016\n",
            "LSTM Epoch 13/20, Batch 1800, Loss: 0.5069\n",
            "LSTM Epoch 13/20, Batch 1900, Loss: 0.4935\n",
            "LSTM Epoch 13/20, Batch 2000, Loss: 0.5130\n",
            "LSTM Epoch 13/20, Batch 2100, Loss: 0.4846\n",
            "LSTM Epoch 13/20, Batch 2200, Loss: 0.5253\n",
            "LSTM Epoch 13/20, Batch 2300, Loss: 0.4843\n",
            "LSTM Epoch 13/20, Batch 2400, Loss: 0.4843\n",
            "LSTM Epoch 13/20, Batch 2500, Loss: 0.5106\n",
            "LSTM Epoch 13/20, Batch 2600, Loss: 0.5018\n",
            "LSTM Epoch 13 average loss: 0.4964\n",
            "LSTM Epoch 14/20, Batch 0, Loss: 0.4504\n",
            "LSTM Epoch 14/20, Batch 100, Loss: 0.4923\n",
            "LSTM Epoch 14/20, Batch 200, Loss: 0.4565\n",
            "LSTM Epoch 14/20, Batch 300, Loss: 0.4704\n",
            "LSTM Epoch 14/20, Batch 400, Loss: 0.4775\n",
            "LSTM Epoch 14/20, Batch 500, Loss: 0.4735\n",
            "LSTM Epoch 14/20, Batch 600, Loss: 0.4775\n",
            "LSTM Epoch 14/20, Batch 700, Loss: 0.4637\n",
            "LSTM Epoch 14/20, Batch 800, Loss: 0.4748\n",
            "LSTM Epoch 14/20, Batch 900, Loss: 0.4774\n",
            "LSTM Epoch 14/20, Batch 1000, Loss: 0.4499\n",
            "LSTM Epoch 14/20, Batch 1100, Loss: 0.4896\n",
            "LSTM Epoch 14/20, Batch 1200, Loss: 0.4631\n",
            "LSTM Epoch 14/20, Batch 1300, Loss: 0.4931\n",
            "LSTM Epoch 14/20, Batch 1400, Loss: 0.4491\n",
            "LSTM Epoch 14/20, Batch 1500, Loss: 0.4365\n",
            "LSTM Epoch 14/20, Batch 1600, Loss: 0.4899\n",
            "LSTM Epoch 14/20, Batch 1700, Loss: 0.4842\n",
            "LSTM Epoch 14/20, Batch 1800, Loss: 0.5035\n",
            "LSTM Epoch 14/20, Batch 1900, Loss: 0.4663\n",
            "LSTM Epoch 14/20, Batch 2000, Loss: 0.5061\n",
            "LSTM Epoch 14/20, Batch 2100, Loss: 0.4953\n",
            "LSTM Epoch 14/20, Batch 2200, Loss: 0.4929\n",
            "LSTM Epoch 14/20, Batch 2300, Loss: 0.4924\n",
            "LSTM Epoch 14/20, Batch 2400, Loss: 0.4692\n",
            "LSTM Epoch 14/20, Batch 2500, Loss: 0.4597\n",
            "LSTM Epoch 14/20, Batch 2600, Loss: 0.4913\n",
            "LSTM Epoch 14 average loss: 0.4732\n",
            "LSTM Epoch 15/20, Batch 0, Loss: 0.4544\n",
            "LSTM Epoch 15/20, Batch 100, Loss: 0.4359\n",
            "LSTM Epoch 15/20, Batch 200, Loss: 0.4878\n",
            "LSTM Epoch 15/20, Batch 300, Loss: 0.4326\n",
            "LSTM Epoch 15/20, Batch 400, Loss: 0.4357\n",
            "LSTM Epoch 15/20, Batch 500, Loss: 0.4741\n",
            "LSTM Epoch 15/20, Batch 600, Loss: 0.4478\n",
            "LSTM Epoch 15/20, Batch 700, Loss: 0.4452\n",
            "LSTM Epoch 15/20, Batch 800, Loss: 0.4554\n",
            "LSTM Epoch 15/20, Batch 900, Loss: 0.4536\n",
            "LSTM Epoch 15/20, Batch 1000, Loss: 0.4352\n",
            "LSTM Epoch 15/20, Batch 1100, Loss: 0.4509\n",
            "LSTM Epoch 15/20, Batch 1200, Loss: 0.4422\n",
            "LSTM Epoch 15/20, Batch 1300, Loss: 0.4444\n",
            "LSTM Epoch 15/20, Batch 1400, Loss: 0.4478\n",
            "LSTM Epoch 15/20, Batch 1500, Loss: 0.4803\n",
            "LSTM Epoch 15/20, Batch 1600, Loss: 0.4537\n",
            "LSTM Epoch 15/20, Batch 1700, Loss: 0.4567\n",
            "LSTM Epoch 15/20, Batch 1800, Loss: 0.4227\n",
            "LSTM Epoch 15/20, Batch 1900, Loss: 0.4341\n",
            "LSTM Epoch 15/20, Batch 2000, Loss: 0.4567\n",
            "LSTM Epoch 15/20, Batch 2100, Loss: 0.4565\n",
            "LSTM Epoch 15/20, Batch 2200, Loss: 0.4560\n",
            "LSTM Epoch 15/20, Batch 2300, Loss: 0.4297\n",
            "LSTM Epoch 15/20, Batch 2400, Loss: 0.4889\n",
            "LSTM Epoch 15/20, Batch 2500, Loss: 0.4540\n",
            "LSTM Epoch 15/20, Batch 2600, Loss: 0.4466\n",
            "LSTM Epoch 15 average loss: 0.4548\n",
            "LSTM Epoch 16/20, Batch 0, Loss: 0.4301\n",
            "LSTM Epoch 16/20, Batch 100, Loss: 0.4181\n",
            "LSTM Epoch 16/20, Batch 200, Loss: 0.4217\n",
            "LSTM Epoch 16/20, Batch 300, Loss: 0.4333\n",
            "LSTM Epoch 16/20, Batch 400, Loss: 0.4554\n",
            "LSTM Epoch 16/20, Batch 500, Loss: 0.4169\n",
            "LSTM Epoch 16/20, Batch 600, Loss: 0.4364\n",
            "LSTM Epoch 16/20, Batch 700, Loss: 0.5029\n",
            "LSTM Epoch 16/20, Batch 800, Loss: 0.4149\n",
            "LSTM Epoch 16/20, Batch 900, Loss: 0.4198\n",
            "LSTM Epoch 16/20, Batch 1000, Loss: 0.4505\n",
            "LSTM Epoch 16/20, Batch 1100, Loss: 0.4561\n",
            "LSTM Epoch 16/20, Batch 1200, Loss: 0.4457\n",
            "LSTM Epoch 16/20, Batch 1300, Loss: 0.4582\n",
            "LSTM Epoch 16/20, Batch 1400, Loss: 0.4220\n",
            "LSTM Epoch 16/20, Batch 1500, Loss: 0.4474\n",
            "LSTM Epoch 16/20, Batch 1600, Loss: 0.4350\n",
            "LSTM Epoch 16/20, Batch 1700, Loss: 0.4559\n",
            "LSTM Epoch 16/20, Batch 1800, Loss: 0.4496\n",
            "LSTM Epoch 16/20, Batch 1900, Loss: 0.4374\n",
            "LSTM Epoch 16/20, Batch 2000, Loss: 0.4225\n",
            "LSTM Epoch 16/20, Batch 2100, Loss: 0.4373\n",
            "LSTM Epoch 16/20, Batch 2200, Loss: 0.4448\n",
            "LSTM Epoch 16/20, Batch 2300, Loss: 0.4733\n",
            "LSTM Epoch 16/20, Batch 2400, Loss: 0.4628\n",
            "LSTM Epoch 16/20, Batch 2500, Loss: 0.4624\n",
            "LSTM Epoch 16/20, Batch 2600, Loss: 0.4482\n",
            "LSTM Epoch 16 average loss: 0.4389\n",
            "LSTM Epoch 17/20, Batch 0, Loss: 0.4171\n",
            "LSTM Epoch 17/20, Batch 100, Loss: 0.4046\n",
            "LSTM Epoch 17/20, Batch 200, Loss: 0.4055\n",
            "LSTM Epoch 17/20, Batch 300, Loss: 0.4201\n",
            "LSTM Epoch 17/20, Batch 400, Loss: 0.4571\n",
            "LSTM Epoch 17/20, Batch 500, Loss: 0.4056\n",
            "LSTM Epoch 17/20, Batch 600, Loss: 0.4287\n",
            "LSTM Epoch 17/20, Batch 700, Loss: 0.4314\n",
            "LSTM Epoch 17/20, Batch 800, Loss: 0.4009\n",
            "LSTM Epoch 17/20, Batch 900, Loss: 0.4519\n",
            "LSTM Epoch 17/20, Batch 1000, Loss: 0.4292\n",
            "LSTM Epoch 17/20, Batch 1100, Loss: 0.4117\n",
            "LSTM Epoch 17/20, Batch 1200, Loss: 0.4545\n",
            "LSTM Epoch 17/20, Batch 1300, Loss: 0.4007\n",
            "LSTM Epoch 17/20, Batch 1400, Loss: 0.4064\n",
            "LSTM Epoch 17/20, Batch 1500, Loss: 0.4134\n",
            "LSTM Epoch 17/20, Batch 1600, Loss: 0.4376\n",
            "LSTM Epoch 17/20, Batch 1700, Loss: 0.4172\n",
            "LSTM Epoch 17/20, Batch 1800, Loss: 0.4544\n",
            "LSTM Epoch 17/20, Batch 1900, Loss: 0.4342\n",
            "LSTM Epoch 17/20, Batch 2000, Loss: 0.4596\n",
            "LSTM Epoch 17/20, Batch 2100, Loss: 0.4495\n",
            "LSTM Epoch 17/20, Batch 2200, Loss: 0.4212\n",
            "LSTM Epoch 17/20, Batch 2300, Loss: 0.4675\n",
            "LSTM Epoch 17/20, Batch 2400, Loss: 0.4521\n",
            "LSTM Epoch 17/20, Batch 2500, Loss: 0.4319\n",
            "LSTM Epoch 17/20, Batch 2600, Loss: 0.4157\n",
            "LSTM Epoch 17 average loss: 0.4258\n",
            "LSTM Epoch 18/20, Batch 0, Loss: 0.4145\n",
            "LSTM Epoch 18/20, Batch 100, Loss: 0.4024\n",
            "LSTM Epoch 18/20, Batch 200, Loss: 0.4350\n",
            "LSTM Epoch 18/20, Batch 300, Loss: 0.4198\n",
            "LSTM Epoch 18/20, Batch 400, Loss: 0.3940\n",
            "LSTM Epoch 18/20, Batch 500, Loss: 0.4056\n",
            "LSTM Epoch 18/20, Batch 600, Loss: 0.4513\n",
            "LSTM Epoch 18/20, Batch 700, Loss: 0.4164\n",
            "LSTM Epoch 18/20, Batch 800, Loss: 0.4043\n",
            "LSTM Epoch 18/20, Batch 900, Loss: 0.4437\n",
            "LSTM Epoch 18/20, Batch 1000, Loss: 0.4167\n",
            "LSTM Epoch 18/20, Batch 1100, Loss: 0.3644\n",
            "LSTM Epoch 18/20, Batch 1200, Loss: 0.4211\n",
            "LSTM Epoch 18/20, Batch 1300, Loss: 0.3790\n",
            "LSTM Epoch 18/20, Batch 1400, Loss: 0.4340\n",
            "LSTM Epoch 18/20, Batch 1500, Loss: 0.3821\n",
            "LSTM Epoch 18/20, Batch 1600, Loss: 0.4041\n",
            "LSTM Epoch 18/20, Batch 1700, Loss: 0.4470\n",
            "LSTM Epoch 18/20, Batch 1800, Loss: 0.3986\n",
            "LSTM Epoch 18/20, Batch 1900, Loss: 0.4005\n",
            "LSTM Epoch 18/20, Batch 2000, Loss: 0.4257\n",
            "LSTM Epoch 18/20, Batch 2100, Loss: 0.4062\n",
            "LSTM Epoch 18/20, Batch 2200, Loss: 0.4210\n",
            "LSTM Epoch 18/20, Batch 2300, Loss: 0.4444\n",
            "LSTM Epoch 18/20, Batch 2400, Loss: 0.4227\n",
            "LSTM Epoch 18/20, Batch 2500, Loss: 0.3891\n",
            "LSTM Epoch 18/20, Batch 2600, Loss: 0.3743\n",
            "LSTM Epoch 18 average loss: 0.4146\n",
            "LSTM Epoch 19/20, Batch 0, Loss: 0.4209\n",
            "LSTM Epoch 19/20, Batch 100, Loss: 0.3740\n",
            "LSTM Epoch 19/20, Batch 200, Loss: 0.4061\n",
            "LSTM Epoch 19/20, Batch 300, Loss: 0.3954\n",
            "LSTM Epoch 19/20, Batch 400, Loss: 0.4112\n",
            "LSTM Epoch 19/20, Batch 500, Loss: 0.3724\n",
            "LSTM Epoch 19/20, Batch 600, Loss: 0.4036\n",
            "LSTM Epoch 19/20, Batch 700, Loss: 0.4306\n",
            "LSTM Epoch 19/20, Batch 800, Loss: 0.3969\n",
            "LSTM Epoch 19/20, Batch 900, Loss: 0.3829\n",
            "LSTM Epoch 19/20, Batch 1000, Loss: 0.4264\n",
            "LSTM Epoch 19/20, Batch 1100, Loss: 0.4152\n",
            "LSTM Epoch 19/20, Batch 1200, Loss: 0.4121\n",
            "LSTM Epoch 19/20, Batch 1300, Loss: 0.4326\n",
            "LSTM Epoch 19/20, Batch 1400, Loss: 0.4274\n",
            "LSTM Epoch 19/20, Batch 1500, Loss: 0.4329\n",
            "LSTM Epoch 19/20, Batch 1600, Loss: 0.4459\n",
            "LSTM Epoch 19/20, Batch 1700, Loss: 0.4221\n",
            "LSTM Epoch 19/20, Batch 1800, Loss: 0.4237\n",
            "LSTM Epoch 19/20, Batch 1900, Loss: 0.4166\n",
            "LSTM Epoch 19/20, Batch 2000, Loss: 0.4322\n",
            "LSTM Epoch 19/20, Batch 2100, Loss: 0.4177\n",
            "LSTM Epoch 19/20, Batch 2200, Loss: 0.4166\n",
            "LSTM Epoch 19/20, Batch 2300, Loss: 0.3907\n",
            "LSTM Epoch 19/20, Batch 2400, Loss: 0.4545\n",
            "LSTM Epoch 19/20, Batch 2500, Loss: 0.3884\n",
            "LSTM Epoch 19/20, Batch 2600, Loss: 0.4219\n",
            "LSTM Epoch 19 average loss: 0.4045\n",
            "LSTM Epoch 20/20, Batch 0, Loss: 0.3647\n",
            "LSTM Epoch 20/20, Batch 100, Loss: 0.4022\n",
            "LSTM Epoch 20/20, Batch 200, Loss: 0.3768\n",
            "LSTM Epoch 20/20, Batch 300, Loss: 0.3860\n",
            "LSTM Epoch 20/20, Batch 400, Loss: 0.3782\n",
            "LSTM Epoch 20/20, Batch 500, Loss: 0.4004\n",
            "LSTM Epoch 20/20, Batch 600, Loss: 0.3898\n",
            "LSTM Epoch 20/20, Batch 700, Loss: 0.4092\n",
            "LSTM Epoch 20/20, Batch 800, Loss: 0.4177\n",
            "LSTM Epoch 20/20, Batch 900, Loss: 0.3794\n",
            "LSTM Epoch 20/20, Batch 1000, Loss: 0.3987\n",
            "LSTM Epoch 20/20, Batch 1100, Loss: 0.3939\n",
            "LSTM Epoch 20/20, Batch 1200, Loss: 0.4048\n",
            "LSTM Epoch 20/20, Batch 1300, Loss: 0.4177\n",
            "LSTM Epoch 20/20, Batch 1400, Loss: 0.4092\n",
            "LSTM Epoch 20/20, Batch 1500, Loss: 0.4124\n",
            "LSTM Epoch 20/20, Batch 1600, Loss: 0.3815\n",
            "LSTM Epoch 20/20, Batch 1700, Loss: 0.4054\n",
            "LSTM Epoch 20/20, Batch 1800, Loss: 0.4062\n",
            "LSTM Epoch 20/20, Batch 1900, Loss: 0.3896\n",
            "LSTM Epoch 20/20, Batch 2000, Loss: 0.4040\n",
            "LSTM Epoch 20/20, Batch 2100, Loss: 0.4121\n",
            "LSTM Epoch 20/20, Batch 2200, Loss: 0.3873\n",
            "LSTM Epoch 20/20, Batch 2300, Loss: 0.3946\n",
            "LSTM Epoch 20/20, Batch 2400, Loss: 0.4112\n",
            "LSTM Epoch 20/20, Batch 2500, Loss: 0.3913\n",
            "LSTM Epoch 20/20, Batch 2600, Loss: 0.4099\n",
            "LSTM Epoch 20 average loss: 0.3959\n"
          ]
        }
      ],
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=embedding_dim,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        embed = self.embedding(x)\n",
        "        output, hidden = self.lstm(embed, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size)\n",
        "        return (h0, c0)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 128\n",
        "hidden_size = 256\n",
        "num_layers = 2\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_lstm = LSTMModel(vocab_size, embedding_dim, hidden_size, num_layers).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model_lstm.parameters(), lr=0.001)\n",
        "num_epochs = 20\n",
        "\n",
        "model_lstm.train()\n",
        "for epoch in range(num_epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        current_batch_size = inputs.size(0)\n",
        "        hidden = model_lstm.init_hidden(current_batch_size)\n",
        "        hidden = (hidden[0].to(device), hidden[1].to(device))\n",
        "        outputs, hidden = model_lstm(inputs, hidden)\n",
        "        outputs = outputs.reshape(-1, vocab_size)\n",
        "        targets = targets.reshape(-1)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"LSTM Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
        "    avg_loss = epoch_loss / len(dataloader)\n",
        "    print(f\"LSTM Epoch {epoch+1} average loss: {avg_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Сгенерированный текст LSTM:\n",
            " Alice was beginning to get very tired of eating and drinking and playing whist with his neighbours five times a week than upon family affection or any thing that home affords '' emma could not like what\n"
          ]
        }
      ],
      "source": [
        "def generate_text_lstm(model, start_text, vocab, inv_vocab, device, gen_length=30):\n",
        "    model.eval()\n",
        "    tokens = word_tokenize(start_text)\n",
        "    input_indices = [vocab.get(token, 0) for token in tokens]\n",
        "    input_tensor = torch.tensor(input_indices, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(input_tensor.size(0))\n",
        "    hidden = (hidden[0].to(device), hidden[1].to(device))\n",
        "    generated_tokens = tokens.copy()\n",
        "    for _ in range(gen_length):\n",
        "        outputs, hidden = model(input_tensor, hidden)\n",
        "        logits = outputs[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        next_token_idx = torch.multinomial(probs, num_samples=1).item()\n",
        "        generated_tokens.append(inv_vocab[next_token_idx])\n",
        "        input_tensor = torch.tensor([[next_token_idx]], dtype=torch.long).to(device)\n",
        "    return \" \".join(generated_tokens)\n",
        "\n",
        "inv_vocab = {idx: token for token, idx in vocab.items()}\n",
        "generated_lstm = generate_text_lstm(model_lstm, \"Alice was beginning to get very tired\", vocab, inv_vocab, device, gen_length=30)\n",
        "print(\"Сгенерированный текст LSTM:\\n\", generated_lstm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "alice was beginning to get very tired you will be so kind as to give her your arm. -- mr elton and miss hawkins -- good morning to you '' emma alone with her father had half\n"
          ]
        }
      ],
      "source": [
        "BeamEntry = namedtuple('BeamEntry', ['sequence', 'hidden', 'sum_logprob'])\n",
        "\n",
        "def generate_text_beam(\n",
        "    model,\n",
        "    start_text,\n",
        "    vocab,\n",
        "    inv_vocab,\n",
        "    device,\n",
        "    max_len=30,\n",
        "    beam_width=5,\n",
        "    alpha=0.7,\n",
        "    repetition_penalty=1.1,\n",
        "    ngram_block=3,\n",
        "    temperature=1.0\n",
        "):\n",
        "    model.eval()\n",
        "    init_tokens = [vocab.get(t, 0) for t in word_tokenize(start_text.lower())]\n",
        "    hidden = model.init_hidden(1)\n",
        "    hidden = (hidden[0].to(device), hidden[1].to(device))\n",
        "    beam = [BeamEntry(init_tokens, hidden, 0.0)]\n",
        "    for _ in range(max_len):\n",
        "        cand = []\n",
        "        for seq, h, s in beam:\n",
        "            last = torch.tensor([[seq[-1]]], dtype=torch.long).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits, h_new = model(last, h)\n",
        "            logits = logits[:, -1, :].squeeze(0)\n",
        "            for t in seq:\n",
        "                logits[t] /= repetition_penalty\n",
        "            if temperature != 1.0:\n",
        "                logits = logits / temperature\n",
        "            log_probs = F.log_softmax(logits, dim=-1)\n",
        "            topk_logp, topk_idx = torch.topk(log_probs, beam_width)\n",
        "            prev_ngrams = {\n",
        "                tuple(seq[i : i + ngram_block])\n",
        "                for i in range(len(seq) - ngram_block + 1)\n",
        "            }\n",
        "            for lp, idx in zip(topk_logp.tolist(), topk_idx.tolist()):\n",
        "                nseq = seq + [idx]\n",
        "                if tuple(nseq[-ngram_block:]) in prev_ngrams:\n",
        "                    continue\n",
        "                cand.append(\n",
        "                    BeamEntry(nseq, (h_new[0].clone(), h_new[1].clone()), s + lp)\n",
        "                )\n",
        "        if not cand:\n",
        "            break\n",
        "        cand.sort(\n",
        "            key=lambda e: e.sum_logprob\n",
        "            / ((5 + len(e.sequence)) ** alpha / 5**alpha),\n",
        "            reverse=True,\n",
        "        )\n",
        "        beam = cand[: beam_width]\n",
        "    best = max(\n",
        "        beam,\n",
        "        key=lambda e: e.sum_logprob\n",
        "        / ((5 + len(e.sequence)) ** alpha / 5**alpha),\n",
        "    )\n",
        "    return \" \".join(inv_vocab.get(i, \"<unk>\") for i in best.sequence)\n",
        "\n",
        "generated_beam = generate_text_beam(\n",
        "    model_lstm,\n",
        "    \"Alice was beginning to get very tired\",\n",
        "    vocab,\n",
        "    {v: k for k, v in vocab.items()},\n",
        "    device,\n",
        "    max_len=30,\n",
        "    beam_width=5,\n",
        ")\n",
        "print(generated_beam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4BCLpxKiXzx"
      },
      "source": [
        "Дополнительная задача, на дополнительный бал:\n",
        "\n",
        "Попытаться реализовать\n",
        "\n",
        "Exhaustive Search\n",
        "\n",
        "Exhaustive Search проверяет все возможные варианты, гарантируя нахождение абсолютно лучшего решения. Однако, это непрактично для большинства реальных задач из-за огромной вычислительной сложности.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5YCuQ1Xg_pE"
      },
      "outputs": [],
      "source": [
        "def dynamic_temperature(timestep, total_timesteps, initial_temp, final_temp):\n",
        "    \"\"\"\n",
        "    Динамическая настройка температуры.\n",
        "\n",
        "    Аргументы:\n",
        "    timestep: Текущий временной шаг.\n",
        "    total_timesteps: Общее количество временных шагов.\n",
        "    initial_temp: Начальная температура.\n",
        "    final_temp: Конечная температура.\n",
        "\n",
        "    Возвращает:\n",
        "    Значение температуры на данном временном шаге.\n",
        "    \"\"\"\n",
        "    temp = initial_temp + (final_temp - initial_temp) * (timestep / total_timesteps)\n",
        "    return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOOxyfyshCo2"
      },
      "outputs": [],
      "source": [
        "def evaluate_bleu(reference, candidate):\n",
        "    \"\"\"\n",
        "    Оценивает качество перевода с помощью метрики BLEU.\n",
        "\n",
        "    Аргументы:\n",
        "    reference: Список ссылок.\n",
        "    candidate: Генерируемый перевод.\n",
        "\n",
        "    Возвращает:\n",
        "    Оценка BLEU.\n",
        "    \"\"\"\n",
        "    bleu_score = sentence_bleu(reference, candidate)\n",
        "    return bleu_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flW1t3lWffsR"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def weighted_greedy_sampling(logits, beams, weights=None):\n",
        "    \"\"\"\n",
        "    Алгоритм взвешенной жадной выборки токенов.\n",
        "\n",
        "    Аргументы:\n",
        "    logits: Логиты токенов (вероятности).\n",
        "    beams: Количество выбираемых токенов.\n",
        "    weights: Веса для каждого токена (по умолчанию равны 1).\n",
        "\n",
        "    Возвращает:\n",
        "    Индексы выбранных токенов.\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        weights = torch.ones_like(logits)\n",
        "\n",
        "    # Нормализуем веса\n",
        "    normalized_weights = weights / weights.sum()\n",
        "\n",
        "    # Применяем взвешивание к логитам\n",
        "    weighted_logits = logits * normalized_weights\n",
        "\n",
        "    # Выбираем лучшие токены\n",
        "    return torch.topk(weighted_logits, beams).indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHE-sNOWgnWs"
      },
      "outputs": [],
      "source": [
        "def beam_search_with_length_penalty(input_ids, node, bar, length, beams, sampling, temperature=0.1, alpha=0.6):\n",
        "    if length == 0:\n",
        "        return None\n",
        "\n",
        "    outputs = model(input_ids)\n",
        "    predictions = outputs.logits\n",
        "\n",
        "    logits = predictions[0, -1, :]\n",
        "\n",
        "    if sampling == 'weighted_greedy':\n",
        "        top_token_ids = weighted_greedy_sampling(logits, beams)\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown sampling method {sampling}\")\n",
        "\n",
        "    for j, token_id in enumerate(top_token_ids):\n",
        "        bar.update(1)\n",
        "\n",
        "        # Рассчитываем вероятность токена\n",
        "        token_score = get_log_prob(logits, token_id)\n",
        "        cumulative_score = graph.nodes[node]['cumscore'] + token_score\n",
        "\n",
        "        # Добавляем предсказанный токен к списку входных идентификаторов\n",
        "        new_input_ids = torch.cat([input_ids, token_id.unsqueeze(0).unsqueeze(0)], dim=-1)\n",
        "\n",
        "        # Учет длины последовательности при расчете общей оценки\n",
        "        sequence_length = len(new_input_ids.squeeze())\n",
        "        length_penalty = ((sequence_length + 5) / 6)**alpha\n",
        "        penalized_cumscore = cumulative_score / length_penalty\n",
        "\n",
        "        # Обновляем граф\n",
        "        token = tokenizer.decode(token_id, skip_special_tokens=True)\n",
        "        current_node = list(graph.successors(node))[j]\n",
        "        graph.nodes[current_node]['tokenscore'] = np.exp(token_score) * 100\n",
        "        graph.nodes[current_node]['cumscore'] = cumulative_score\n",
        "        graph.nodes[current_node]['penalized_cumscore'] = penalized_cumscore\n",
        "        graph.nodes[current_node]['sequencescore'] = 1/(len(new_input_ids.squeeze())) * cumulative_score\n",
        "        graph.nodes[current_node]['token'] = token + f\"_{length}_{j}\"\n",
        "\n",
        "        # Рекурсивный вызов\n",
        "        beam_search_with_length_penalty(\n",
        "            new_input_ids,\n",
        "            current_node,\n",
        "            bar,\n",
        "            length-1,\n",
        "            beams,\n",
        "            sampling,\n",
        "            temperature,\n",
        "            alpha\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tSrUqRdjhzg"
      },
      "outputs": [],
      "source": [
        "def get_best_sequence(G):\n",
        "    # Create a list of leaf nodes\n",
        "    leaf_nodes = [node for node in G.nodes() if G.out_degree(node)==0]\n",
        "\n",
        "    # Get the leaf node with the highest cumscore\n",
        "    max_score_node = None\n",
        "    max_score = float('-inf')\n",
        "    for node in leaf_nodes:\n",
        "        if G.nodes[node]['sequencescore'] > max_score:\n",
        "            max_score = G.nodes[node]['sequencescore']\n",
        "            max_score_node = node\n",
        "\n",
        "    # Retrieve the sequence of nodes from this leaf node to the root node in a list\n",
        "    path = nx.shortest_path(G, source=0, target=max_score_node)\n",
        "\n",
        "    # Return the string of token attributes of this sequence\n",
        "    sequence = \"\".join([G.nodes[node]['token'].split('_')[0] for node in path])\n",
        "\n",
        "    return sequence, max_score\n",
        "\n",
        "sequence, max_score = get_best_sequence(graph)\n",
        "print(f\"Generated text: {sequence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjJNC4J8g8lw"
      },
      "outputs": [],
      "source": [
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super().__init__()\n",
        "        self.query_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.key_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value_proj = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, query, keys, values):\n",
        "        Q = self.query_proj(query)\n",
        "        K = self.key_proj(keys)\n",
        "        V = self.value_proj(values)\n",
        "        attention_scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(K.shape[-1])\n",
        "        attention_probs = F.softmax(attention_scores, dim=-1)\n",
        "        context_vector = torch.matmul(attention_probs, V)\n",
        "        return context_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pJPEL-PhFYJ"
      },
      "outputs": [],
      "source": [
        "def add_regularization(model, regularization_type='dropout'):\n",
        "    \"\"\"\n",
        "    Добавляет регуляризацию к модели.\n",
        "\n",
        "    Аргументы:\n",
        "    model: Модель, к которой добавляется регуляризация.\n",
        "    regularization_type: Тип регуляризации ('dropout' или другой).\n",
        "\n",
        "    Возвращает:\n",
        "    Регуляризированную модель.\n",
        "    \"\"\"\n",
        "    if regularization_type == 'dropout':\n",
        "        model.add_module('dropout', nn.Dropout(p=0.1))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mb3LbhXchG8B"
      },
      "outputs": [],
      "source": [
        "def add_regularization(model, regularization_type='dropout'):\n",
        "    \"\"\"\n",
        "    Добавляет регуляризацию к модели.\n",
        "\n",
        "    Аргументы:\n",
        "    model: Модель, к которой добавляется регуляризация.\n",
        "    regularization_type: Тип регуляризации ('dropout' или другой).\n",
        "\n",
        "    Возвращает:\n",
        "    Регуляризированную модель.\n",
        "    \"\"\"\n",
        "    if regularization_type == 'dropout':\n",
        "        model.add_module('dropout', nn.Dropout(p=0.1))\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQCnXffWgrLF"
      },
      "outputs": [],
      "source": [
        "def rank_sequences(graph, nodes):\n",
        "    \"\"\"\n",
        "    Ранжирование последовательностей по различным критериям.\n",
        "\n",
        "    Аргументы:\n",
        "    graph: Граф, содержащий узлы с информацией о токенах.\n",
        "    nodes: Список узлов, для которых производится ранжирование.\n",
        "\n",
        "    Возвращает:\n",
        "    Отсортированный список узлов по убыванию их общей оценки.\n",
        "    \"\"\"\n",
        "    scores = []\n",
        "    for node in nodes:\n",
        "        cumscore = graph.nodes[node]['cumscore']\n",
        "        sequencescore = graph.nodes[node]['sequencescore']\n",
        "        diversity_score = calculate_diversity_score(graph, node)\n",
        "        final_score = cumscore * sequencescore * diversity_score\n",
        "        scores.append((node, final_score))\n",
        "\n",
        "    sorted_nodes = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    return [node for node, _ in sorted_nodes]\n",
        "\n",
        "def calculate_diversity_score(graph, node):\n",
        "    \"\"\"\n",
        "    Расчет показателя разнообразия последовательности.\n",
        "\n",
        "    Аргументы:\n",
        "    graph: Граф, содержащий узлы с информацией о токенах.\n",
        "    node: Узел, для которого рассчитывается показатель разнообразия.\n",
        "\n",
        "    Возвращает:\n",
        "    Показатель разнообразия последовательности.\n",
        "    \"\"\"\n",
        "    tokens = []\n",
        "    while node != 0:\n",
        "        tokens.append(graph.nodes[node]['token'])\n",
        "        node = list(graph.predecessors(node))[0]\n",
        "    unique_tokens = set(tokens)\n",
        "    diversity_score = len(unique_tokens) / len(tokens)\n",
        "    return diversity_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpnQHXSVg6U4"
      },
      "outputs": [],
      "source": [
        "def diverse_top_k_sampling(logits, k, diversity_rate=0.7):\n",
        "    \"\"\"\n",
        "    Выборка токенов с учетом диверсификации.\n",
        "\n",
        "    Аргументы:\n",
        "    logits: Логиты токенов.\n",
        "    k: Количество выбираемых токенов.\n",
        "    diversity_rate: Коэффициент диверсификации (чем ближе к 1, тем сильнее влияние).\n",
        "\n",
        "    Возвращает:\n",
        "    Индексы отобранных токенов.\n",
        "    \"\"\"\n",
        "    top_values, top_indices = torch.topk(logits, k)\n",
        "    diversity_scores = compute_diversity_scores(top_indices)\n",
        "    combined_scores = top_values * (diversity_rate * diversity_scores + (1 - diversity_rate))\n",
        "    _, selected_indices = torch.topk(combined_scores, k)\n",
        "    return top_indices[selected_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw078z_AgtnY"
      },
      "outputs": [],
      "source": [
        "def train_on_the_fly(model, input_ids, labels, optimizer, loss_fn):\n",
        "    \"\"\"\n",
        "    Обучение модели на лету.\n",
        "\n",
        "    Аргументы:\n",
        "    model: Модель, которую нужно обновить.\n",
        "    input_ids: Входные данные.\n",
        "    labels: Метки для входных данных.\n",
        "    optimizer: Оптимизатор для обучения.\n",
        "    loss_fn: Функция потерь.\n",
        "\n",
        "    Возвращает:\n",
        "    Потери после одного шага обучения.\n",
        "    \"\"\"\n",
        "    outputs = model(input_ids)\n",
        "    predictions = outputs.logits\n",
        "    loss = loss_fn(predictions.view(-1, predictions.size(-1)), labels.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QakCdw1Ogv27"
      },
      "outputs": [],
      "source": [
        "def adaptive_temperature_adjustment(current_step, max_steps, initial_temp, min_temp):\n",
        "    \"\"\"\n",
        "    Адаптивное управление температурой.\n",
        "\n",
        "    Аргументы:\n",
        "    current_step: Текущий шаг процесса.\n",
        "    max_steps: Максимальное количество шагов.\n",
        "    initial_temp: Начальная температура.\n",
        "    min_temp: Минимальная температура.\n",
        "\n",
        "    Возвращает:\n",
        "    Температуру на текущем шаге.\n",
        "    \"\"\"\n",
        "    progress = current_step / max_steps\n",
        "    temp = initial_temp * (1 - progress) + min_temp * progress\n",
        "    return temp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKortT93joFF"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "\n",
        "# Создаем граф с весовыми коэффициентами на рёбрах и дополнительными параметрами для узлов\n",
        "G = nx.DiGraph()\n",
        "\n",
        "# Пример добавления узлов с дополнительным параметром 'context'\n",
        "G.add_node('A', sequencescore=10, context='Начало предложения')\n",
        "G.add_node('B', sequencescore=20, context='Основная часть предложения')\n",
        "G.add_node('C', sequencescore=30, context='Конец предложения')\n",
        "\n",
        "# Добавляем рёбра с весами\n",
        "G.add_edge('A', 'B', weight=2)\n",
        "G.add_edge('B', 'C', weight=4)\n",
        "G.add_edge('A', 'C', weight=8)\n",
        "\n",
        "# Функция для поиска оптимального пути через граф с учетом весов\n",
        "def get_best_sequence_with_weights(G):\n",
        "    # Получаем листовые узлы (листья)\n",
        "    leaf_nodes = [node for node in G.nodes() if G.out_degree(node) == 0]\n",
        "\n",
        "    # Находим листовой узел с максимальным score\n",
        "    best_leaf_node = None\n",
        "    best_score = float('-inf')\n",
        "    for node in leaf_nodes:\n",
        "        if G.nodes[node]['sequencescore'] > best_score:\n",
        "            best_score = G.nodes[node]['sequencescore']\n",
        "            best_leaf_node = node\n",
        "\n",
        "    # Используем алгоритм Дейкстры для нахождения кратчайшего пути до листового узла\n",
        "    try:\n",
        "        shortest_path = nx.dijkstra_path(G, source='A', target=best_leaf_node, weight='weight')\n",
        "    except nx.NetworkXNoPath:\n",
        "        print(\"Нет пути от начального узла до листового!\")\n",
        "        return None, None\n",
        "\n",
        "    # Формируем последовательность текста\n",
        "    sequence = ''.join([G.nodes[node]['token'].split('_')[0] for node in shortest_path])\n",
        "\n",
        "    return sequence, best_score\n",
        "\n",
        "# Вызываем функцию\n",
        "sequence, max_score = get_best_sequence_with_weights(G)\n",
        "if sequence is not None:\n",
        "    print(f\"Оптимальная последовательность: {sequence}, максимальная оценка: {max_score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "new",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
